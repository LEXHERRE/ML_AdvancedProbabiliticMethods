{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec2f2cbb158c2d8f03d616a694899ec7",
     "grade": false,
     "grade_id": "cell-81c5a400584e4a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2021)\n",
    "\n",
    "Pekka Marttinen, Santosh Hiremath, Tianyu Cui, Yogesh Kumar, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Shaoxiong Ji, Alexander Nikitin, Sebastiaan De Peuter, Joakim JÃ¤rvinen.\n",
    "\n",
    "## Exercise 3, due on Tuesday February 9 at 23:00.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Poisson-Gamma\n",
    "2. Problem 2: Multivariate Gaussian\n",
    "3. Problem 3: Posterior of regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bb2e5ebde49e1760a076b099d6e5a6",
     "grade": false,
     "grade_id": "cell-573bbaa2ef327be0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 1: Poisson-Gamma\n",
    "\n",
    "Suppose you have $N$ i.i.d. observations $\\mathbf{x}= \\{x_i\\}_{i=1}^N$ from a $\\operatorname{Poisson}(\\lambda)$ distribution with a rate parameter $\\lambda$ that has a conjugate prior \n",
    "\n",
    "$$\\lambda \\sim \\operatorname{Gamma}(a,b)$$\n",
    "\n",
    "with the shape and rate hyperparameters $a$ and $b$. Derive the posterior distribution $\\lambda|\\bf{x}$.\n",
    "\n",
    "Write your solutions in LateX or attach a picture in the answer cell provided below. You can add a picture using the command ```!(imagename_in_the_folder.jpg)```. Latex in here works similarly as you would write it normally! You can use some of the definitions from the exercise description as a reference. The list of valid Latex commands in Jypyter notebook can be found here: http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "We can derive the posterior as follows:\n",
    "\n",
    "$$p(\\lambda|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\lambda)p(\\lambda)}{p(\\mathbf{x})}$$\n",
    "\n",
    "By dropping the elements that are independent to $\\lambda$ we get that:\n",
    "$$p(\\lambda|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\lambda)p(\\lambda)}{p(\\mathbf{x})}\\propto p(\\mathbf{x}|\\lambda)p(\\lambda)$$\n",
    "\n",
    "Then knowing that the $\\operatorname{Poisson}(\\lambda)$ prior distribution follows the function:\n",
    "\n",
    "$$p(\\mathbf{x}|\\lambda) = \\prod_{i=1}^{N}\\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!}$$\n",
    "\n",
    "And the density of the $\\operatorname{Gamma}(a,b)$ distribution is:\n",
    "\n",
    "$$p(\\lambda) = \\frac{b^a\\lambda^{a-1}e^{-b\\lambda}}{\\Gamma(a)}$$\n",
    "\n",
    "We get that:\n",
    "$$p(\\mathbf{x}|\\lambda)p(\\lambda) = \\prod_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!} \\times \\frac{b^a\\lambda^{a-1}e^{-b\\lambda}}{\\Gamma(a)}$$\n",
    "\n",
    "Again pruning the independent elements to $\\lambda$ we get that:\n",
    "\n",
    "$$p(\\mathbf{x}|\\lambda)p(\\lambda) = \\prod_{i=1}^{N} \\frac{e^{-\\lambda}\\lambda^{x_i}}{x_i!} \\times \\frac{b^a\\lambda^{a-1}e^{-b\\lambda}}{\\Gamma(a)} \\propto e^{-N\\lambda}\\lambda^{\\sum_{i=1}^N x_i} \\times \\lambda^{a-1}e^{-b\\lambda}=$$\n",
    "$$= e^{-\\lambda(N+b)}\\lambda^{a+\\sum_{i=1}^N x_i-1} \\propto \\operatorname{Gamma}(\\lambda|a_n,b_n)$$\n",
    "having that $a_n = a + \\sum_{i=1}^Nx_i$ and $b_n = b+N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d1bd8470ba33c5aa2596654e3cefbc",
     "grade": false,
     "grade_id": "cell-7fdfccb96ae5c3d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Problem 2: Multivariate Gaussian\n",
    "\n",
    "Suppose we have $N$ i.i.d. observations $\\mathbf{X} = \\{\\mathbf{x}_i\\}_{i=1}^N$ from a multivariate Gaussian distribution $$\\mathbf{x}_i \\mid \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$ with unknown mean parameter $\\boldsymbol{\\mu}$  and a known covariance matrix $\\boldsymbol{\\Sigma}$. As prior information on the mean parameter we have $$ \\boldsymbol{\\mu} \\sim \\mathcal{N}(\\mathbf{m_0}, \\mathbf{S_0}). $$\n",
    "\n",
    "__(a)__ Derive the posterior distribution $p(\\boldsymbol{\\mu}|\\mathbf{X})$ of the mean parameter $\\boldsymbol{\\mu}$. Write your solution in LateX or attach a picture of the solution in the cell below.\n",
    "\n",
    "__(b)__ Compare the Bayesian estimate (posterior mean) to the maximum likelihood estimate by generating $N=10$ observations from the bivariate Gaussian \n",
    "        $$\\mathcal{N}\\left(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\right).$$\n",
    "For this you can use the Python function [numpy.random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html), making use of the fact that the elements of the bivariate random vectors are independent. In the Bayesian case, use the prior with $\\mathbf{m_0} = [0,0]^T$ and $\\mathbf{S_0} = [\\begin{smallmatrix}0.1 & 0 \\\\ 0 & 0.1\\end{smallmatrix}]$. Report both estimates. Is the Bayesian estimate closer to the true value $\\boldsymbol{\\mu} = [0,0]^T$? Use the code template given below (after the answer cell) to complete your answer.\n",
    "\n",
    "Write your solutions to __(a)__ and __(b)__ in LateX or attach a picture in the answer cell provided below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "__(a)__\n",
    "\n",
    "As done in the previous exercise \n",
    "\n",
    "$$p(\\boldsymbol{\\mu}|X)\n",
    "=\n",
    "\\frac{p(X|\\boldsymbol{\\mu})p(\\boldsymbol{\\mu})}{p(X)}\n",
    "\\propto\n",
    "p(X|\\boldsymbol{\\mu})p(\\boldsymbol{\\mu})$$\n",
    "\n",
    "We also know that the multivariate Gaussian disrtibutions $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ and $\\mathcal{N}(\\mathbf{m_0}, \\mathbf{S_0})$ follow the functions:\n",
    "\n",
    "$$p(x_i|\\boldsymbol{\\mu})\n",
    "=\n",
    "(2\\pi)^{-\\frac{D}{2}}|\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "$$p(\\boldsymbol{\\mu})=\n",
    "(2\\pi)^{-\\frac{D}{2}}|\\mathbf{S_0}|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\mathbf{m_0})^\\top \\mathbf{S_0}^{-1}(\\boldsymbol{\\mu}-\\mathbf{m_0})\\right)$$\n",
    "\n",
    "So we have that:\n",
    "\n",
    "$$p(\\boldsymbol{\\mu}|X)\n",
    "\\propto\n",
    "p(X|\\boldsymbol{\\mu})p(\\boldsymbol{\\mu}) =$$\n",
    "\n",
    "$$= \\prod_{i=1}^N (2\\pi)^{-\\frac{D}{2}}|\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\\right) \\times (2\\pi)^{-\\frac{D}{2}}|\\mathbf{S_0}|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\mathbf{m_0})^\\top \\mathbf{S_0}^{-1}(\\boldsymbol{\\mu}-\\mathbf{m_0})\\right) \\propto$$\n",
    "\n",
    "$$\\propto \\prod_{i=1}^N \\exp\\left(-\\frac{1}{2}(x_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\\right) \\times \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\mathbf{m_0})^\\top \\mathbf{S_0}^{-1}(\\boldsymbol{\\mu}-\\mathbf{m_0})\\right) =$$\n",
    "\n",
    "\n",
    "$$= \\exp\\left(\\sum_{i=1}^N -\\frac{1}{2}(x_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\\right) \\times \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\mathbf{m_0})^\\top \\mathbf{S_0}^{-1}(\\boldsymbol{\\mu}-\\mathbf{m_0})\\right)$$\n",
    "\n",
    "Now for easier visualization we will operate both elements separately: $\\exp(E1)\\times\\exp(E2)$\n",
    "\n",
    "#### E1:\n",
    "$$\\sum_{i=1}^N -\\frac{1}{2}(x_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(x_i-\\boldsymbol{\\mu})\n",
    "=\n",
    "-\\frac{1}{2}\\sum_{i=1}^N \\left(x_i^\\top\\boldsymbol{\\Sigma}^{-1}x_i - x_i^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} - \\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}x_i + \\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}\\right) \\propto$$\n",
    "\n",
    "$$\\propto -\\frac{1}{2}N\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} -\\frac{1}{2}\\sum_{i=1}^N \\left(- 2\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}x_i\\right)\n",
    "=\n",
    "-\\frac{1}{2}N\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} +\\sum_{i=1}^N \\left(\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}x_i\\right)$$\n",
    "\n",
    "#### E2:\n",
    "$$-\\frac{1}{2}(\\boldsymbol{\\mu}-\\mathbf{m_0})^\\top \\mathbf{S_0}^{-1}(\\boldsymbol{\\mu}-\\mathbf{m_0})\n",
    "=\n",
    "-\\frac{1}{2}\\left(\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu}-\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0} - \\mathbf{m_0}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu} + \\mathbf{m_0}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right) \\propto$$\n",
    "\n",
    "$$\\propto -\\frac{1}{2}\\left(\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu} - 2\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)\n",
    "=\n",
    "-\\frac{1}{2}\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu} + \\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0}$$\n",
    "\n",
    "So we have that:\n",
    "\n",
    "$$p(\\boldsymbol{\\mu}|X)\n",
    "\\propto\n",
    "\\exp\\left(-\\frac{1}{2}N\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} +\\sum_{i=1}^N \\left(\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}x_i\\right)\\right) \\times \\exp\\left(-\\frac{1}{2}\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu} + \\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right) =$$\n",
    "\n",
    "$$= \\exp\\left(-\\frac{1}{2}N\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} +\\sum_{i=1}^N \\left(\\boldsymbol{\\mu}^\\top\\boldsymbol{\\Sigma}^{-1}x_i\\right)-\\frac{1}{2}\\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\boldsymbol{\\mu} + \\boldsymbol{\\mu}^\\top \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)=$$\n",
    "\n",
    "$$=\\exp\\left(-\\left[\\frac{1}{2}\\boldsymbol{\\mu}^\\top\\left(N\\boldsymbol{\\Sigma}^{-1} + \\mathbf{S_0}^{-1}\\right)\\boldsymbol{\\mu} - \\boldsymbol{\\mu}^\\top\\left(\\sum_{i=1}^N \\left(\\boldsymbol{\\Sigma}^{-1}x_i\\right) + \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)\\right]\\right)$$\n",
    "\n",
    "And completing the square we obtain:\n",
    "\n",
    "$$\\exp\\left(-\\left[\\frac{1}{2}\\boldsymbol{\\mu}^\\top\\left(N\\boldsymbol{\\Sigma}^{-1} + \\mathbf{S_0}^{-1}\\right)\\boldsymbol{\\mu} - \\boldsymbol{\\mu}^\\top\\left(\\sum_{i=1}^N \\left(\\boldsymbol{\\Sigma}^{-1}x_i\\right) + \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)\\right]\\right) =$$\n",
    "\n",
    "$$=\\exp\\left(-\\left[\\frac{1}{2}\\left(\\boldsymbol{\\mu} - (N\\boldsymbol{\\Sigma}^{-1}+\\mathbf{S_0}^{-1})^{-1} \\left(\\boldsymbol{\\Sigma}^{-1}\\sum_{i=1}^N x_i + \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)\\right)^\\top (N\\boldsymbol{\\Sigma}^{-1}+\\mathbf{S_0}^{-1}) \\left(\\boldsymbol{\\mu} - (N\\boldsymbol{\\Sigma}^{-1}+\\mathbf{S_0}^{-1})^{-1}\\left(\\boldsymbol{\\Sigma}^{-1}\\sum_{i=1}^N x_i + \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)\\right) \\right]\\right)$$\n",
    "\n",
    "And we can observe that:\n",
    "\n",
    "$$p(\\boldsymbol{\\mu}|X) \\propto \\mathcal{N}(\\boldsymbol{\\mu}_n,\\boldsymbol{\\Sigma}_n)$$\n",
    "\n",
    "having that $\\boldsymbol{\\mu}_n = (N\\boldsymbol{\\Sigma}^{-1}+\\mathbf{S_0}^{-1})^{-1}\\left(\\boldsymbol{\\Sigma}^{-1}\\sum_{i=1}^N x_i + \\mathbf{S_0}^{-1}\\mathbf{m_0}\\right)$ and $\\boldsymbol{\\Sigma}_n = (N\\boldsymbol{\\Sigma}^{-1}+\\mathbf{S_0}^{-1})^{-1}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af88913931d4649db8324917756a5b72",
     "grade": false,
     "grade_id": "cell-e6a09ef8bf1f72d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13161913  0.22786068]\n",
      "[-0.06580957  0.11393034]\n",
      "The Bayesian estimate is colser\n"
     ]
    }
   ],
   "source": [
    "# template for 2(b)\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "mean = np.array([0, 0])\n",
    "S0 = np.array([[0.1, 0],[0, 0.1]])\n",
    "Sigma = np.array([[1, 0],[0, 1]])\n",
    "m0 = np.array([0,0]).T\n",
    "N = 10\n",
    "\n",
    "# Sample N bivariate normal vectors\n",
    "# compute MLE and also the posterior mean solution\n",
    "\n",
    "# x = ? #EXERCISE\n",
    "# mle = ? #EXERCISE\n",
    "# posterior_mean = ? #EXERCISE \n",
    "\n",
    "# YOUR CODE HERE\n",
    "np.random.seed(5)\n",
    "x = np.random.multivariate_normal(mean, sigma, 10)\n",
    "\n",
    "mle = np.mean(x,axis=0)\n",
    "print(mle)\n",
    "\n",
    "def post_mean(N,x,Sigma,S0,m0):\n",
    "    invSigma = np.linalg.inv(Sigma)\n",
    "    invS0 = np.linalg.inv(S0)\n",
    "    Sum = np.array([np.sum(x.T[0]),np.sum(x.T[1])])\n",
    "    return np.linalg.inv(N*invSigma + invS0)@(invSigma@Sum + invS0@m0)\n",
    "                     \n",
    "posterior_mean = post_mean(N,x,Sigma,S0,m0)\n",
    "print(posterior_mean)\n",
    "\n",
    "err_mle = np.mean(mle)\n",
    "err_post = np.mean(posterior_mean)\n",
    "if err_mle > err_post:\n",
    "    print('The Bayesian estimate is colser')\n",
    "else:\n",
    "    print('The maximum likelihood estimate is colser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__\n",
    "\n",
    "As we can see the implemented code give the following estimations:\n",
    "$$\\boldsymbol{\\mu}_{mle} = [0.00113893\\;0.0697555]^\\top$$\n",
    "$$\\boldsymbol{\\mu}_{post\\_mean} = [0.00056947\\;0.03487775]^\\top$$\n",
    "\n",
    "And the Bayesian estimate is closer to the real value $\\boldsymbol{\\mu} = [0,0]^\\top$.\n",
    "\n",
    "But the difference is not much, and in some cases the likelihood estimate is closer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddf1e85bf2fabec6a07c3676a5945499",
     "grade": false,
     "grade_id": "cell-6f265c79745ea700",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: Posterior of regression weights\n",
    "\n",
    "Suppose $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$. Assume a prior $$\\mathbf{w} \\sim \\mathcal{N} (\\mathbf{0},\\alpha^{-1}\\mathbf{I}).$$ Use 'completing the square' to show that the posterior of $\\mathbf{w}$ is given by $p(\\mathbf{w} \\mid \\mathbf{y}, \\mathbf{x}, \\alpha, \\beta)=\\mathcal{N}(\\mathbf{w} \\mid \\mathbf{m}, \\mathbf{S}),$ where \n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left( \\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T \\right)^{-1}\\;, \\\\\n",
    "    \\mathbf{m} &= \\beta \\mathbf{S} \\sum_{i=1}^{n} y_i \\mathbf{x}_i.\n",
    "\\end{align*}\n",
    "\n",
    "Write your solution in LateX or attach a picture of the solution in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "First of all, having that $y_{i}=\\mathbf{w}^{T}\\mathbf{x}_{i}+\\epsilon_{i},$ for $i=1,\\ldots,n,$ where $\\epsilon_{i}\\sim \\mathcal{N}(0,\\beta^{-1})$, we know that:\n",
    "    \n",
    "$$y_i \\sim \\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x_i},\\beta^{-1})$$\n",
    "\n",
    "From now on, we will define $D$ as data.\n",
    "\n",
    "we know that the likelihood is:\n",
    "\n",
    "$$p(D|\\mathbf{w}) \\propto \\exp\\left(-\\frac{1}{2}(y-\\mathbf{w}^\\top \\mathbf{A})^\\top(\\beta^{-1})(y-\\mathbf{w}^\\top \\mathbf{A})\\right)$$\n",
    "\n",
    "where $\\mathbf{A}= [\\mathbf{x_1}^\\top,...,\\mathbf{x_n}^\\top]^\\top$.\n",
    "\n",
    "\n",
    "And the prior is:\n",
    "\n",
    "$$p(\\mathbf{w}) \\propto \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}\\right)$$\n",
    "\n",
    "So the posterior is:\n",
    "    \n",
    "$$p(\\mathbf{w}|D) \\propto p(D|\\mathbf{w})p(\\mathbf{w}) = \\exp\\left(-\\frac{1}{2}(y-\\mathbf{w}^\\top \\mathbf{A})^\\top(\\beta^{-1})(y-\\mathbf{w}^\\top \\mathbf{A})\\right) \\times \\exp\\left(-\\frac{1}{2}\\mathbf{w}^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}\\right) \\propto$$\n",
    "\n",
    "$$\\propto \\exp\\left(-\\left(\\frac{1}{2}\\mathbf{w}^\\top \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)\\mathbf{w} -y^\\top\\beta \\mathbf{A}^{-1} \\mathbf{w}\\right)\\right)$$\n",
    "\n",
    "And now completing the square we obtain:\n",
    "\n",
    "$$\\exp\\left(-\\left(\\frac{1}{2}\\mathbf{w}^\\top \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)\\mathbf{w} -y^\\top\\beta \\mathbf{A}^{-1} \\mathbf{w}\\right)\\right) \\propto$$\n",
    "\n",
    "$$\\propto \\exp\\left(-\\left(\\frac{1}{2}\\left(\\mathbf{w}-\\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1}y^\\top\\beta \\mathbf{A}^{-1}\\right)^\\top \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right) \\left(\\mathbf{w}-\\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1}y^\\top\\beta \\mathbf{A}^{-1}\\right)\n",
    "                           -\\frac{1}{2}\\left(y^\\top\\beta \\mathbf{A}^{-1})^\\top\\right) \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1} y^\\top\\beta \\mathbf{A}^{-1}\\right)\\right) \\propto$$\n",
    "\n",
    "$$\\propto \\left(-\\left(\\frac{1}{2}\\left(\\left(\\mathbf{w}-(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1} y^\\top\\beta \\mathbf{A}^{-1}\\right)^\\top \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right) \\left(\\mathbf{w} - \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1}y^\\top\\beta \\mathbf{A}^{-1}\\right)\\right)\\right)$$\n",
    "                \n",
    "Hence, we can observe that $P(\\mathbf{w}|D) \\sim \\operatorname{Gaussian}(\\mathbf{m},\\mathbf{S})$ where:\n",
    "                \n",
    "$$\\mathbf{S} = \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1} = \\left(\\beta\\sum_{i=1}^N \\mathbf{x_i}\\mathbf{x_i}^\\top + \\alpha \\mathbf{I}\\right)^{-1}$$\n",
    "\n",
    "$$\\mathbf{m} = \\left(\\mathbf{A}^{-1}\\beta \\mathbf{A} + \\alpha \\mathbf{I}\\right)^{-1}y^\\top\\beta \\mathbf{A}^{-1} = \\beta S \\sum_{i=1}^N \\mathbf{x_i}y_i$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
