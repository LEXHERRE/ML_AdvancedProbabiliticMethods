{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2847fa55ab6e21bb9edebcd216ff3e12",
     "grade": false,
     "grade_id": "cell-5b335005bb36ae92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2021)\n",
    "\n",
    "Pekka Marttinen, Santosh Hiremath, Tianyu Cui, Yogesh Kumar, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Shaoxiong Ji, Alexander Nikitin, Sebastiaan De Peuter, Joakim JÃ¤rvinen.\n",
    "\n",
    "## Exercise 5, due on Tuesday February 23 at 23:00.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: EM for missing observations\n",
    "2. Problem 2: Extension of 'simple example' from the lecture\n",
    "3. Problem 3: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18c12b98afa6a333b6b4717029202b7d",
     "grade": false,
     "grade_id": "cell-298bb2ed1de6d806",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: EM for missing observations\n",
    "Suppose random variables $X_{i}$ follow a bivariate normal distribution $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$, where\n",
    "$ \\Sigma = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix} $.\n",
    "\n",
    "Suppose further that we have observations on $X_{1}=(X_{11},X_{12})^{T}$, $X_{2}=(X_{21},X_{22})^{T}$ and $X_{3}=(X_{31},X_{32})^{T}$, such that\n",
    "$X_{1}$ and $X_{3}$ are fully observed, and from $X_{2}$ we have observed only\n",
    "the second coordinate. Thus, our data matrix can be written as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12}\\\\\n",
    "? & x_{22}\\\\\n",
    "x_{31} & x_{32}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "where the rows correspond to the transposed observations $\\mathbf{x}_{1}^{T},\\mathbf{x}_{2}^{T},\\mathbf{x}_{3}^{T}$. Suppose we want to learn the unknown parameter $\\rho$ using the EM-algorithm. Denote the missing observation by $Z$ and derive the E-step of the algorithm, i.e., __(a)__ write the complete data log-likelihood $\\ell(\\rho)$, __(b)__ compute the posterior distribution of the missing observation, given the observed variables and current estimate for $\\rho$, and __(c)__ evaluate the expectation of $\\ell(\\rho)$ with respect to the posterior distribution of the missing observations.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "1. In general, for $X \\sim \\mathcal{N}_2(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $X=(X_1, X_2)^{T}$, $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2)^{T}$ and $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix}$, \n",
    "we have \n",
    "$$ X_1 \\mid X_2 = x_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right),$$  with $\\rho$ being the correlation coefficient.\n",
    "2. For evaluating the expectation of $\\ell(\\rho)$, you can make use of the following two rules: \n",
    "    - $\\mathbf{x_2}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{x_2} = trace(\\boldsymbol{\\Sigma}^{-1}\\mathbf{x_2x_2^T}).$\n",
    "    - if $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $\\langle{X^2}\\rangle = \\mu^2 + \\sigma^2$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "__(a)__\n",
    "\n",
    "$$\\ell(\\rho) = \\log \\prod_{i=1}^3 p(x_i|\\rho) = \\sum_{i=1}^3 \\log p(x_i|\\rho) = \\sum_{i=1}^3 \\log \\mathcal{N}_{2}(x_i|0,\\Sigma) = \\sum_{i=1}^3 \\log \\frac{\\det \\Sigma^{-\\frac{1}{2}}}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x_i^\\top \\Sigma^{-1}x_i\\right) =$$\n",
    "$$= -\\frac{1}{2}x_1^\\top\\Sigma^{-1}x_1-\\frac{1}{2}x_2^\\top\\Sigma^{-1}x_2-\\frac{1}{2}x_3^\\top\\Sigma^{-1}x_3 -\\frac{3}{2}\\log(1-\\rho^2) + C$$\n",
    "where $C$ is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__\n",
    "\n",
    "First of all we know that $X_{i}\\sim \\mathcal{N}_{2}(0,\\Sigma)$ where,\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n",
    "            \\sigma_1^{2} & \\rho\\sigma_{1}\\sigma_{2} \\\\ \n",
    "            \\rho\\sigma_{1}\\sigma_{2} & \\sigma_2^{2} \n",
    "            \\end{pmatrix} = \\begin{bmatrix} 1 & \\rho\\\\ \\rho & 1 \\end{bmatrix}$$\n",
    "            \n",
    "so we obtain that $\\mu_1 = \\mu_2 = 0$ and $\\sigma_1 = \\sigma_2 = 1$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$p(Z|x_{22}) = \\mathcal{N}\\left(Z|\\mu_1 + \\frac{\\sigma_1}{\\sigma_2}\\rho(x_2-\\mu_2), (1-\\rho^2)\\sigma_1^{2}\\right) = \\mathcal{N}\\left(Z|0 + \\frac{1}{1}\\rho(x_{22}-0), (1-\\rho^2)1^{2}\\right) = \\mathcal{N}\\left(Z|\\rho x_{22}, (1-\\rho^2)\\right)$$\n",
    "\n",
    "So we obtain that $E(Z) =  \\rho x_{22}$ and $E(Z^2) = \\rho^2x_{22}^2 + 1-\\rho^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__\n",
    "\n",
    "We have that,\n",
    "\n",
    "$$Q(\\rho,\\rho_0) = E_{Z|X,\\rho_0}\\ell(\\rho) = E\\left(-\\frac{1}{2}x_1^\\top\\Sigma^{-1}x_1-\\frac{1}{2}x_2^\\top\\Sigma^{-1}x_2-\\frac{1}{2}x_3^\\top\\Sigma^{-1}x_3 -\\frac{3}{2}\\log(1-\\rho^2) + C\\right)$$\n",
    "\n",
    "and knowing that $\\ell(\\rho)$ only depends on the expectation of $x_2^\\top\\Sigma^{-1}x_2$ because the missing value belongs to $x_2$ we obtain that,\n",
    "\n",
    "$$\\DeclareMathOperator{\\Tr}{Tr}\n",
    "Q(\\rho,\\rho_0) = E\\left(-\\frac{1}{2}x_2^\\top\\Sigma^{-1}x_2 + C\\right) = -\\frac{1}{2}E\\left(x_2^\\top\\Sigma^{-1}x_2\\right) + C = -\\frac{1}{2}\\Tr\\left(\\Sigma^{-1}x_2x_2^\\top\\right) + C =$$\n",
    "\n",
    "$$= -\\frac{1}{2}\\Tr\\left(\\Sigma^{-1}\\begin{bmatrix} E(Z^2) & E(Z)x_{22}\\\\ E(Z)x_{22} & x_{22}^2 \\end{bmatrix}\\right) + C = -\\frac{1}{2}\\Tr\\left(\\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}\\cdot\\begin{bmatrix} \\rho^2x_{22}^2+1-\\rho^2 & \\rho x_{22}^2\\\\ \\rho x_{22}^2 & x_{22}^2 \\end{bmatrix}\\right) + C =$$\n",
    "\n",
    "$$= -\\frac{1}{2}\\Tr\\left(\\frac{1}{1-\\rho^2}\\begin{bmatrix}1-\\rho^2 & \\dots \\\\ \\dots & -\\rho^2x_{22}^2 + x_{22}^2\\end{bmatrix}\\right) + C = -\\frac{1}{2}(1+x_{22}^2) + C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cd36c403dde3a532a877a43ad92522",
     "grade": false,
     "grade_id": "cell-46bf29d7d4d92271",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: Extension of 'simple example' from the lecture\n",
    "Suppose that we have $N$ independent observations $x = ( x_1, \\dots, x_N )$ from a two-component mixture of univariate Gaussian distributions with unknown mixing co-efficients and unknown mean of the second component:\n",
    "$$ p(x_{n} \\mid \\theta,\\tau)=(1-\\tau)\\mathcal{N}(x_{n}|0,1)+\\tau\\mathcal{N}(x_{n} \\mid \\theta,1).$$\n",
    "\n",
    "**(a)** Write down the complete data log-likelihood and derive the EM-algorithm for learning the maximum likelihood estimates for $\\theta$ and $\\tau$. \n",
    "\n",
    "**(b)** Simulate some data from the model ($N = 100$ samples) with the true values of parameters $\\theta$ = 3 and $\\tau = 0.5$. Run your EM algorithm to see whether the learned parameters converge close to the true values (by e.g. just listing the estimates from a few iterations or plotting them). Use the code template below (after the answer cell) as a starting point. \n",
    "\n",
    "**HINT**: The E and M steps for simple example.pdf from the lecture material looks as follows\n",
    "```Python\n",
    "\t# E-step: compute the responsibilities r2 for component 2\n",
    "\tr1_unnorm = scipy.stats.norm.pdf(x, 0, 1)\n",
    "\tr2_unnorm = scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "\tr2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\t\n",
    "\t# M-step: compute the parameter value that maximizes\n",
    "\t# the expectation of the complete-data log-likelihood.\n",
    "\ttheta[it] = sum(r2 * x) / sum(r2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta} = \\arg \\max_\\theta\\{\\log p(x|\\theta,\\tau)\\}$$\n",
    "\n",
    "$$\\hat{\\tau} = \\arg \\max_\\tau\\{\\log p(x|\\theta,\\tau)\\}$$\n",
    "\n",
    "We formulate the model using the **latent variable representation**.\n",
    "\n",
    "$$z_n = (z_{n1},z_{n2})^\\top =  \n",
    "\\begin{cases}\n",
    "    (1,0)^\\top, & \\left(x_n \\text{ is form } \\mathcal{N}(x_n|0,1)\\right)\\\\\n",
    "    (0,1)^\\top, & \\left(x_n \\text{ is form } \\mathcal{N}(x_n|\\theta,1)\\right)\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "In the EM-algorithm we will maximize the expectation of the **log-likelihood of the complete data** $(\\mathbf{x},\\mathbf{z})$:\n",
    "\n",
    "$$\\log p(x,z|\\theta,\\tau) = \\log\\left\\{\\prod_{n=1}^Np(x_n,z_n|\\theta,\\tau)\\right\\} = \\sum_{n=1}^N \\log p(x_n,z_n|\\theta,\\tau) =$$\n",
    "\n",
    "$$= \\sum_{n=1}^N \\log\\left(\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right)^{z_{n1}}\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)^{z_{n2}}\\right) =$$\n",
    "\n",
    "$$= \\sum_{n=1}^N \\left\\{z_{n1}\\log\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right) + z_{n2}\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right\\}$$\n",
    "\n",
    "\n",
    "**E-step** $1^0$: Compute the posterior distribution of the latent variables, given the current estimate $\\theta_0$ of $\\theta$ and $\\tau_0$ of $\\tau$:\n",
    "\n",
    "$$p(z_{n1}=1|x_n,\\theta_0,\\tau_0) \\propto p(z_{n1}=1,\\tau_0)p(x_n|z_n,\\theta_0,\\tau_0) = (1-\\tau)\\mathcal{N}(x_{n}|0,1)$$\n",
    "\n",
    "$$p(z_{n2}=1|x_n,\\theta_0,\\tau_0) \\propto p(z_{n2}=1,\\tau_0)p(x_n|z_n,\\theta_0,\\tau_0) = \\tau\\mathcal{N}(x_{n}|\\theta,1)$$\n",
    "\n",
    "\n",
    "By normalizing this two last equations we get\n",
    "\n",
    "$$\\gamma(z_{n2}) = p(z_{n2}=1|x_n,\\theta_0,\\tau_0) = \\frac{\\tau\\mathcal{N}(x_{n}|\\theta,1)}{(1-\\tau)\\mathcal{N}(x_{n}|0,1) + \\tau\\mathcal{N}(x_{n}|\\theta,1)}$$\n",
    "\n",
    "**E-step** $2^0$: Evaluate the expectation of the complete data log-likelyhood over the posterior distribution of the latent variables:\n",
    "\n",
    "$$Q(\\theta,\\tau,\\theta_0,\\tau_0) = E_{\\mathbf{z}|\\mathbf{x},\\theta_0,\\tau_0}\\left[\\log p(\\mathbf{x},\\mathbf{z}|\\theta)\\right]\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\sum_{n=1}^N \\left\\{E[z_{n1}]\\log\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right) + E[z_{n2}]\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right\\}\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\sum_{n=1}^N \\left\\{[1-\\gamma(z_{n2})]\\log\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right) + \\gamma(z_{n2})\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right\\}$$\n",
    "\n",
    "**M-Step:** Maximize $Q(\\theta,\\tau,\\theta_0,\\tau_0)$ with respect to $\\theta$ and and with respect to $\\tau$.\n",
    "\n",
    "First derive $Q(\\theta,\\tau,\\theta_0,\\tau_0)$ with respect to $\\theta$:\n",
    "\n",
    "$$\\frac{d}{d\\theta} Q(\\theta,\\tau,\\theta_0,\\tau_0)\n",
    "=\n",
    "\\frac{d}{d\\theta} \\sum_{n=1}^N \\left\\{[1-\\gamma(z_{n2})]\\log\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right) + \\gamma(z_{n2})\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right\\}\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\frac{d}{d\\theta} \\sum_{n=1}^N \\gamma(z_{n2})\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\n",
    "=\n",
    "\\frac{d}{d\\theta} \\sum_{n=1}^N \\gamma(z_{n2})\\left(\\log\\left(\\tau\\right) + \\log\\left(\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right)\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\frac{d}{d\\theta} \\sum_{n=1}^N \\gamma(z_{n2})\\log\\left(\\mathcal{N}(x_{n}|\\theta,1)\\right)\n",
    "=\n",
    "\\sum_{n=1}^N \\gamma(z_{n2})\\log\\left(x_n-\\theta\\right)$$\n",
    "\n",
    "Now, setting $\\frac{d}{d\\theta} Q(\\theta,\\tau,\\theta_0,\\tau_0) = 0$ we get:\n",
    "\n",
    "$$\\theta = \\frac{\\sum_{n=1}^N\\gamma(z_{n2})x_n}{\\sum_{n=1}^N\\gamma(z_{n2})} = \\frac{1}{N_2}\\sum_{n=1}^N\\gamma(z_{n2})x_n$$\n",
    "\n",
    "where $N_2 = \\sum_{n=1}^N\\gamma(z_{n2})$ wich can be interpreted as the effective number of observations assigned to component 2.\n",
    "\n",
    "Now we derive $Q(\\theta,\\tau,\\theta_0,\\tau_0)$ with respect to $\\tau$:\n",
    "\n",
    "$$\\frac{d}{d\\tau} Q(\\theta,\\tau,\\theta_0,\\tau_0)\n",
    "=\n",
    "\\frac{d}{d\\tau} \\sum_{n=1}^N \\left\\{[1-\\gamma(z_{n2})]\\log\\left((1-\\tau)\\mathcal{N}(x_{n}|0,1)\\right) + \\gamma(z_{n2})\\log\\left(\\tau\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right\\}\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\frac{d}{d\\tau} \\sum_{n=1}^N \\left\\{[1-\\gamma(z_{n2})]\\left(\\log\\left(1-\\tau\\right) + \\log\\left(\\mathcal{N}(x_{n}|0,1)\\right)\\right) + \\gamma(z_{n2})\\left(\\log\\left(\\tau\\right) + \\log\\left(\\mathcal{N}(x_{n}|\\theta,1)\\right)\\right)\\right\\}\n",
    "= $$\n",
    "\n",
    "$$ =\n",
    "\\frac{d}{d\\tau} \\sum_{n=1}^N \\left\\{[1-\\gamma(z_{n2})]\\log\\left(1-\\tau\\right) + \\gamma(z_{n2})\\log\\left(\\tau\\right)\\right\\}\n",
    "=\n",
    "\\sum_{n=1}^N \\left\\{ -\\frac{1-\\gamma(z_{n2})}{\\log\\left(1-\\tau\\right)} + \\frac{\\gamma(z_{n2})}{\\log\\left(\\tau\\right)}\\right\\}\n",
    "$$\n",
    "\n",
    "Now, setting $\\frac{d}{d\\tau}Q(\\theta,\\tau,\\theta_0,\\tau_0) = 0$ we get:\n",
    "\n",
    "$$\\tau = \\frac{N_2}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b479fd715d72b0417e4f3ac4c0f8e914",
     "grade": false,
     "grade_id": "cell-1abac854e88e7dc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta       tau\n",
      "1.0000000  0.1000000\n",
      "3.2393002  0.3798055\n",
      "3.2787207  0.5396835\n",
      "3.2208416  0.5618515\n",
      "3.2012856  0.5684228\n",
      "3.1949151  0.5705275\n",
      "3.1928396  0.5712102\n",
      "3.1921631  0.5714324\n",
      "3.1919426  0.5715048\n",
      "3.1918707  0.5715284\n",
      "3.1918472  0.5715361\n",
      "3.1918396  0.5715386\n",
      "3.1918371  0.5715394\n",
      "3.1918363  0.5715397\n",
      "3.1918360  0.5715397\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n",
      "3.1918359  0.5715398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkvUlEQVR4nO3de3xU9bnv8c8DBLlLJSgI2KT7IFWgRg33aqlX4AB2V8RbtbTnFBEvhyoeL22RfeqrtYq7troFbeWFrdR7UWGjUq0RFQUDGyxsUKHSJIhcbSCGW8Jz/liTMEkmySSZZCZrvu/Xa71mzfr91ppnluPDL2vW7xlzd0REpPVrk+wAREQkMZTQRURCQgldRCQklNBFREJCCV1EJCTaJeuFMzMzPSsrK1kvLyLSKq1evXq3u/eM1Za0hJ6VlUV+fn6yXl5EpFUys3/U1qZLLiIiIaGELiISEkroIiIhoYQuIhISSugiIiGhhC4iEhJK6CIiIZG0+9Bbmy++gI8+Cpbt22HwYBg+HHr0SHZkIiIBJfQoR47Ap58eS9wVy6ZNsGtX7H0GDIARI2DkyODx9NOhjf7uEZEkSMuEvncvbNxYM2lv2QJlZcf69ewZJOyJE4PHAQPg61+Hk06CtWthxQp47z1YvBgWLAj2Of54GDbsWIIfNizYJiLS3CxZv1iUm5vryZj6/9578M1vwtGjwfOMDOjf/1jCjl5OOCG+Y7rD5s3HEvyKFbB+fbDdDAYNCpJ7xUi+f/9gu4hIQ5nZanfPjdWWdiP0//qvIJk/9RTk5kJWFrRr4lkwC5J0//7w/e8H2/btg5UrjyX4Z56Bxx4L2rp3h969g+vvmZl1P/boAV/5CrRt27QYRST80i6hFxYGo/LJk5v3Wne3bnDhhcECwT8iGzcGCX7NGti5E/bsCUb2K1fC7t3BNfxYzIKkXpHkTzgBOnSA446D9u2PPUavx9pWsZ6REfwD0bZtcA4as1T8hWFW+3o87dHvsSnPG6q1/4XU2uNPd126BDki0dIuoRcUQJ8+Lf/FZZs2MHBgsMTiDiUlQWLfsydYKtarP372GRw6FCyHDwdL9Hr09wAiknpuvx3uvTfxx027hF5YCKeckuwoajKDrl2DJTu7accqLw9G+7Ul/EOHgr8YGruUlwev4x4ssdbjaa/Q1OcNlaSvjRKmtccvcMYZzXPctEvoBQXBl6JhVnE5pUOHZEciIi0pre6YLi+HbdtSc4QuItJUaZXQP/88uL7cr1+yIxERSby0SuiFhcGjRugiEkZpldALCoJHjdBFJIzSKqFrhC4iYZZWCb2gILihX7VVRCSM6k3oZtbBzFaZ2Toz22Bm/xajj5nZb81ss5l9aGZnNU+4TVNxD7pm2YlIGMVzH/oh4Dx3LzGzDOAdM3vF3d+P6jMW6B9ZhgFzI48ppbBQ189FJLzqHaF7oCTyNCOyVJ+rdgnwh0jf94HuZtY7saE2XUGBErqIhFdcM0XNrC2wGvgfwH+4+8pqXfoAhVHPiyLbticiyOpGj665bfJkmD4dSkth3Lia7VdfHRTE6tEj9v7XXw+XXx6M4q+5pmb7rbfChAlB7fTrrqvZ/tOfwgUXBHXSZ8yo2f6LXwSlc1esgLvuqtn+4IOQkwOvvw733FOz/dFHg5K+ixfDAw/UbP/jH4N/rJ55BubOrdn+/PNBca8FC47Vbo+2dCl06gSPPALPPluzPS8veJwzB5YsqdrWsSO88kqw/vOfwxtvVG3v0QNeeCFYv/POoEBZtL594ckng/UZM4JzGO3UU49Vqpw6FT7+uGp7Tk5w/gC+9z0oKqraPmIE/PKXwfqllwb1cKKdfz787GfB+tixcOBA1fbx42HmzGC9MZ+9KVOCZfdumDSpZrs+e+n32at4T4kW15ei7l7u7jlAX2ComQ2q1iXWVekaFSfMbKqZ5ZtZ/q7afgKomezdGzz26dOiLysi0mIa/AMXZnY38KW7z4na9iiQ5+5PRZ5/BIx291pH6C39AxdvvgnnnRf8C37eeS32siIiCVXXD1zEc5dLTzPrHlnvCFwAbKrW7WXg2sjdLsOB4rqSeTJoUpGIhF0819B7A09ErqO3AZ519yVmNg3A3ecBS4FxwGagFPhBM8XbaBWTivr2TW4cIiLNpd6E7u4fAmfG2D4vat2BGxIbWmIVFAQ/+tyxY7IjERFpHmkzUzRVf9hCRCRR0iqh6/q5iIRZ2iR0TSoSkbBLi4ReXAz79+uSi4iEW1okdN2yKCLpIC0Suuqgi0g6SIuErhG6iKSDtEjohYXQti30Trn6jyIiiZMWCb2gICjK1bZtsiMREWk+aZHQNalIRNJBWiR03YMuIukg9An96NGg6LxG6CISdqFP6Dt3wpEjGqGLSPiFPqHrlkURSRehT+iaVCQi6SL0CV0jdBFJF6FP6IWFwS+Kn3BCsiMREWleoU/oFbcsmiU7EhGR5hX6hK5JRSKSLkKf0DWpSETSRagT+uHDsGOHRugikh5CndC3bQN3jdBFJD2EOqHrlkURSSf1JnQz62dmb5rZRjPbYGb/J0af0WZWbGZrI8us5gm3YTSpSETSSbs4+pQBt7r7GjPrCqw2s7+4+39X6/e2u49PfIiNpxG6iKSTekfo7r7d3ddE1vcDG4E+zR1YIhQWQo8ewcQiEZGwa9A1dDPLAs4EVsZoHmFm68zsFTMbWMv+U80s38zyd+3a1fBoG0i3LIpIOok7oZtZF+AFYIa776vWvAb4qrufATwEvBjrGO7+mLvnuntuz549Gxly/DSpSETSSVwJ3cwyCJL5Qnf/c/V2d9/n7iWR9aVAhpllJjTSRtAIXUTSSTx3uRjwOLDR3f+9lj69Iv0ws6GR4+5JZKANtW8fFBdrhC4i6SOeu1xGAdcAfzOztZFtdwGnALj7PGAScL2ZlQEHgCvc3RMfbvwqblnUCF1E0kW9Cd3d3wHqrFXo7g8DDycqqERQQheRdBPamaKaVCQi6Sa0Cb2gANq0gZNPTnYkIiItI7QJvbAwSObt4vmWQEQkBEKb0HXLooikm9AmdE0qEpF0E8qE7h4kdI3QRSSdhDKh79oFhw5phC4i6SWUCV1lc0UkHYUyoesedBFJR6FO6Bqhi0g6CWVCLyiADh0gM+n1HkVEWk4oE3rFHS5WZwUaEZFwCWVC16QiEUlHoZwYX1gIF16Y7ChE0seRI0coKiri4MGDyQ4lNDp06EDfvn3JyMiIe5/QJfQjR+CzzzRCF2lJRUVFdO3alaysLEzXOpvM3dmzZw9FRUVkZ2fHvV/oLrl89lkwU1S3LIq0nIMHD9KjRw8l8wQxM3r06NHgv3hCl9A1qUgkOZTME6sx5zN0CV2TikTSzz//+U8eeeQRAPLy8hg/fnyD9l+wYAGfffZZc4TWokKX0DVCF0k/0Qm9McKS0EP3pWhhIXTvDl26JDsSEWkpd9xxB1u2bCEnJ4eMjAw6d+7MpEmTWL9+PWeffTZPPvkkZsbq1au55ZZbKCkpITMzkwULFvDuu++Sn5/P1VdfTceOHXnvvfe4//77Wbx4MQcOHGDkyJE8+uijreKSUigTui63iCTPjBmwdm1ij5mTAw8+WHv7vffey/r161m7di15eXlccsklbNiwgZNPPplRo0bx7rvvMmzYMG666SZeeuklevbsyTPPPMNPfvIT5s+fz8MPP8ycOXPIzc0F4MYbb2TWrFkAXHPNNSxZsoQJEyYk9k01g9AldE0qEpGhQ4fSt29fAHJycti6dSvdu3dn/fr1XBiZpFJeXk7v3r1j7v/mm29y3333UVpayt69exk4cGA4ErqZ9QP+APQCjgKPuftvqvUx4DfAOKAUmOLuaxIfbv0KC2HkyGS8sohA3SPplnLcccdVrrdt25aysjLcnYEDB/Lee+/Vue/BgweZPn06+fn59OvXj9mzZ7eaCVPxfClaBtzq7qcBw4EbzOz0an3GAv0jy1RgbkKjjNOXX8LevRqhi6Sbrl27sn///jr7DBgwgF27dlUm9CNHjrBhw4Ya+1ck78zMTEpKSnj++eebMfLEqneE7u7bge2R9f1mthHoA/x3VLdLgD+4uwPvm1l3M+sd2bfF6JZFkfTUo0cPRo0axaBBg+jYsSMnnXRSjT7t27fn+eef5+abb6a4uJiysjJmzJjBwIEDmTJlCtOmTav8UvRHP/oRgwcPJisriyFDhiThHTWOBTk4zs5mWcByYJC774vavgS4193fiTx/A7jd3fOr7T+VYATPKaeccvY//vGPJr+BaMuWwcUXw1tvwbnnJvTQIlKHjRs3ctpppyU7jNCJdV7NbLW758bqH/d96GbWBXgBmBGdzCuaY+xS418Kd3/M3XPdPbdnz57xvnTcNEIXkXQWV0I3swyCZL7Q3f8co0sREH3lui/Q4nfpFxQENdD79GnpVxYRSb56E3rkDpbHgY3u/u+1dHsZuNYCw4Hilr5+DsEIvVcvaEC1SRGR0IjnPvRRwDXA38xsbWTbXcApAO4+D1hKcMviZoLbFn+Q8EjjoElFIpLO4rnL5R1iXyOP7uPADYkKqrEKCuAb30h2FCIiyRGa4lzuGqGLSHoLTULfswcOHNCkIpF01NRqi/XJy8tjxYoVlc+nTJnSoAlHzR1fhdAkdN2yKJK+6kqY5eXlTT5+9YTeUEroDaQ66CLpK7p87m233UZeXh7f/va3ueqqqxg8eDBbt25l0KBBlf3nzJnD7NmzAdiyZQtjxozh7LPP5pxzzmHTpk1Vjr1161bmzZvHr3/9a3Jycnj77bcBWL58OSNHjuRrX/taldH6/fffz5AhQ/jGN77B3XffHTO+kpISzj//fM466ywGDx7MSy+9lJDzEJpqixqhi6SO0aNrbps8GaZPh9JSGDeuZvuUKcGyezdMmlS1LS+v7teLLp8b9M9j1apVrF+/nuzsbLZu3VrrvlOnTmXevHn079+flStXMn36dP76179WtmdlZTFt2jS6dOnCzJkzAXj88cfZvn0777zzDps2bWLixIlMmjSJZcuW8cknn7Bq1SrcnYkTJ7J8+fIa8ZWVlbFo0SK6devG7t27GT58OBMnTmxyzfXQJPSCAmjfHpphAqqItEJDhw4lOzu7zj4lJSWsWLGCyy67rHLboUOH4jr+d77zHdq0acPpp5/Ojh07AFi2bBnLli3jzDPPrDz+J598winVRpruzl133cXy5ctp06YN27ZtY8eOHfTq1ashb7GG0CT0wsLgckub0FxEEmm96hpRd+pUd3tmZv0j8nh07ty5cr1du3YcPXq08nlFRcWjR4/SvXv3ypFzQ0SX6K2oieXu3HnnnVx33XVV+lb/C2HhwoXs2rWL1atXk5GRQVZWVkJK9IYm/emHLUTSV33lc0866SR27tzJnj17OHToEEuWLAGgW7duZGdn89xzzwFBQl63bl2Dj1/h4osvZv78+ZSUlACwbds2du7cWWP/4uJiTjzxRDIyMnjzzTdJVKHC0CT0ihG6iKSf6PK5t912W432jIwMZs2axbBhwxg/fjxf//rXK9sWLlzI448/zhlnnMHAgQNjfkE5YcIEFi1aVOVL0VguuugirrrqKkaMGMHgwYOZNGkS+/fvrxHf1VdfTX5+Prm5uSxcuLBKPE3RoPK5iZSbm+v5+fn1d4xDWRl06AB33AH33JOQQ4pIA6h8bvNotvK5qWz7digv1whdRNJbKBK6blkUEQlJQtekIhGRkCR0jdBFREKS0AsKoFu3YBERSVehSOgqmysiEpKErklFIumtpaoZprpQJHRNKhJJb0rogVaf0A8cCKqz6ZKLSPqKLk/74x//OGZp2rpK6IZFqy/OVXGHi0boIqlhxqszWPv52oQeM6dXDg+OebDW9ujytGVlZZSWltYoTZsOQpPQNUIXEai9NG06aPUJXZOKRFJLXSPpllBbadraSuiGSb3X0M1svpntNLP1tbSPNrNiM1sbWWYlPszaVYzQ+/ZtyVcVkVQSXZ62ttK0tZXQDZN4RugLgIeBP9TR5213H5+QiBqooABOOgmias2LSJqJLk87ZMgQNm3aRG5uLjk5OZWlaaNL6GZnZyesZG0qqTehu/tyM8tqgVgaRZOKRATgT3/6U719br75Zm6++eYWiCY5EnXb4ggzW2dmr5jZwNo6mdlUM8s3s/xdu3Yl5IU1qUhEJJCIhL4G+Kq7nwE8BLxYW0d3f8zdc909t2cCfs3ZXZOKREQqNDmhu/s+dy+JrC8FMswss8mRxeGLL+DLL3XJRUQEEpDQzayXmVlkfWjkmHuaetx4aFKRiMgx9X4pamZPAaOBTDMrAu4GMgDcfR4wCbjezMqAA8AV3kI/VKpJRSIix8Rzl8uV9bQ/THBbY4vTpCIRkWNadXGuwkLIyIBevZIdiYgkU3NXW8zLy2PFihXNdvxEadUJvaAA+vSBNq36XYhIU9WV0MvLy5t8fCX0FqBJRSICVcvn3nbbbeTl5fHtb3+bq666isGDB9dZOnfLli2MGTOGs88+m3POOYdNmzZVOfbWrVuZN28ev/71r8nJyeHtt99m8eLFDBs2jDPPPJMLLrigsvjX7NmzmTNnTuW+gwYNYuvWrc3+/iu06uJcBQXwzW8mOwoRqW70gtE1tk0eOJnpQ6ZTeqSUcQvH1WifkjOFKTlT2F26m0nPTqrSljclr87Xiy6fC8GIetWqVaxfv57s7Ow6k+rUqVOZN28e/fv3Z+XKlUyfPp2//vWvle1ZWVlMmzaNLl26MHPmTAC++OIL3n//fcyM3//+99x333088MADdcbYElptQi8vh23bNEIXkdiGDh1KdnZ2nX1KSkpYsWIFl112WeW2Q4cO1XvsoqIiLr/8crZv387hw4frfZ2W0moT+uefQ1mZ7nARSUV1jag7ZXSqsz2zU2a9I/J4dO7cuXK9ttK5R48epXv37pUj+3jddNNN3HLLLUycOJG8vLzKyzfJLtHbaq+ha1KRiFSILp8bS22lc7t160Z2djbPPfccEPw4xrp16+o9fnFxMX369AHgiSeeqNyelZXFmjVrAFizZg2ffvpp099cA7T6hK5LLiISXT73tttuq9EeXTp3/PjxVUrnLly4kMcff5wzzjiDgQMHVv4GabQJEyawaNGiyi9FZ8+ezWWXXcY555xDZuaxSieXXnope/fuJScnh7lz53Lqqac2zxuuhbXQpM4acnNzPT8/v9H7P/AAzJwJe/fCV76SwMBEpME2btzIaaedluwwQifWeTWz1e6eG6t/qx6hd+kC3bsnOxIRkdTQahN6RR30oCyYiIi02oSuSUUiIlW12oSuXyoSSS3J+j4urBpzPltlQj94EHbu1AhdJFV06NCBPXv2KKkniLuzZ88eOnTo0KD9WuXEoqKi4FEjdJHU0LdvX4qKikjUbwVL8I9k3759G7RPq0zomlQkkloyMjJSZvp7OmuVl1wqfthCl1xERI5plQm9YoTewL9GRERCrdUm9J49oWPHZEciIpI6WmVC1y2LIiI1tcqErklFIiI1tcqErhG6iEhN9SZ0M5tvZjvNbH0t7WZmvzWzzWb2oZmdlfgwjykuhv37NUIXEakunhH6AmBMHe1jgf6RZSowt+lh1a7ilkWN0EVEqqo3obv7cmBvHV0uAf7ggfeB7mbWO1EBVqdJRSIisSXiGnofoDDqeVFkWw1mNtXM8s0sv7FThLt0gTFjQJPSRESqSkRCj1WRPGaFHnd/zN1z3T23Z8+ejXqxc8+FV16B3s32N4CISOuUiIReBERfAOkLfJaA44qISAMkIqG/DFwbudtlOFDs7tsTcFwREWmAeqstmtlTwGgg08yKgLuBDAB3nwcsBcYBm4FS4AfNFayIiNSu3oTu7lfW0+7ADQmLSEREGqVVzhQVEZGalNBFREJCCV1EJCSU0EVEQkIJXUQkJJTQRURCQgldRCQklNBFREJCCV1EJCSU0EVEQkIJXUQkJJTQRURCQgldRCQklNBFREJCCV1EJCSU0EVEQkIJXUQkJJTQRURCQgldRCQk6v1NUUl97o7jHPWjDVqi923MevUYYsbWgL517SMSJid0PIETO5+Y8OMqoTcTd+fLI1+y88udNZbdpbs5WHaQw+WHOVR+KHgsO1T5PHq9etvh8sMcLj9M+dHyysSsBCjSutw+6nbuveDehB9XCb2B3J2Nuzeybd+2msm6NHjcUbKDnV/u5EDZgZjH6JzRmY4ZHWnftj3HtT0ueGx3XJX1rsd1rfK8fZtjfTLaZtCuTTvaWJtGLYYFj2YAGNbo9WhGzW1Ag/rWtY9IWJyWeVqzHDeuhG5mY4DfAG2B37v7vdXaRwMvAZ9GNv3Z3f9f4sJMDX//4u/csPQGXt38apXtGW0yOLHziZXLgB4DqjyPXnp26knHjI5JegciEmb1JnQzawv8B3AhUAR8YGYvu/t/V+v6truPb4YYk+5w+WHmrJjDz5f/nIw2Gdx/4f0M7zu8Mkkff9zxGlGKSNLFM0IfCmx2978DmNnTwCVA9YTeYkYvGF1j2+SBk5k+ZDqlR0oZt3BcjfYpOVOYkjOF3aW7mfTspBrt1+dez+WDLqewuJBrFl1Tub34YDEf7/2Y0iOlTDp9EjcNuYlZebNY8vGSKvv/9NyfcsHXLmDt52uZ8eqMGsf/xfm/YGS/kawoXMFdb9xVo/3BMQ+S0yuH1//+Ovcsv6dG+6PjH2VA5gAWf7SYB957oEb7H//1j/Q7vh/PrH+Guflza7Q/P/l5MjtlsmDtAhasXVCjfenVS+mU0YlHPniEZzc8W6M9b0oeAHNWzKnx3jtmdOSVq18B4Odv/Zw3Pn2jSnuPTj14YfILANz5+p28V/Relfa+3fry5HefBGDGqzNY+/naKu2n9jiVxyY8BsDUxVP5eM/HVdpzeuXw4JgHAfjen79H0b6iKu0j+o7glxf8EoBLn72UPaV7qrSfn30+P/vWzwAYu3AsB45UvVQ2/tTxzBw5E2jZz16FW0fcyoQBE/ho90dct+S6Gu367LW+z17Fe0q0eG5b7AMURj0vimyrboSZrTOzV8xsYKwDmdlUM8s3s/xdu3Y1ItyWc+ToET7a/RFrd6yl/Gg5s86dxXOXPcdJXU5KdmgiIjFZfbeQmdllwMXu/r8jz68Bhrr7TVF9ugFH3b3EzMYBv3H3/nUdNzc31/Pz85v8BhLN3Xli3RPMXDaT4kPF3DriVmZ9axadMjolOzQREcxstbvnxmqL55JLEdAv6nlf4LPoDu6+L2p9qZk9YmaZ7r67MQEny6bdm5i2ZBpv/eMtRvUbxbzx8xh04qBkhyUiEpd4EvoHQH8zywa2AVcAV0V3MLNewA53dzMbSnApZ0+NI6WoA0cO8Iu3f8Gv3v0VXdp34XcTfscPz/whbUwTaUWk9ag3obt7mZndCLxGcNvifHffYGbTIu3zgEnA9WZWBhwArvD6ruWkiL9s+QvX/+f1bPliC9d84xrmXDSnWWZwiYg0t7juQ3f3pcDSatvmRa0/DDyc2NCa1+cln3PLa7fw1PqnOLXHqbxx7Rucl31essMSEWm0tJspetSP8mj+o9z5xp0cLDvIv43+N24fdTvHtTsu2aGJiDRJWiV0d2fGqzN4aNVDnJ99PnP/51z696jzZhwRkVYjrRL6Pcvv4aFVD3HL8FuYc9Ecze4UkVBJm9s4HvngEWblzWJKzhQlcxEJpbRI6E+vf5obl97IxAET+d2E3ymZi0gohT6hv7b5Na5ddC3nfPUcnr70adq1SaurTCKSRkKd0N8vep/vPvtdBp44kJeveFlla0Uk1EKb0Dfs3MC4heM4uevJvHr1qxzf4fhkhyQi0qxCmdC3/nMrFz15ER3adWDZ95apQqKIpIXQXVDe+eVOLvrjRZQeKeXtH7xN9leykx2SiEiLCFVC33doH2OeHEPRviJev/Z1VUoUkbQSmoR+sOwglzx9CX/b+TdevuJlRvYbmeyQRERaVCgSetnRMq584Ure2voWC7+7kLH9xyY7JBGRFtfqE7q7c93i63hx04s8NPYhrhx8ZbJDEhFJilZ/l8sdr9/B/LXzuftbd3Pj0BuTHY6ISNK06oR+37v3cd+K+7hhyA3c/a27kx2OiEhStdqEPv+/5nP767dzxaAr+O3Y36o+i4ikvVaZ0F/c9CI/WvwjLv6Xi3niO0/otz9FRGiFCT1vax5XPH8FQ/sM5YXJL9C+bftkhyQikhJaXULv2akn38r6Fv951X/SuX3nZIcjIpIyWt1tiwNPHMhr33st2WGIiKScVjdCFxGR2OJK6GY2xsw+MrPNZnZHjHYzs99G2j80s7MSH6qIiNSl3oRuZm2B/wDGAqcDV5rZ6dW6jQX6R5apwNwExykiIvWIZ4Q+FNjs7n9398PA08Al1fpcAvzBA+8D3c2sd4JjFRGROsST0PsAhVHPiyLbGtoHM5tqZvlmlr9r166GxioiInWIJ6HHmoLpjeiDuz/m7rnuntuzZ8944hMRkTjFk9CLgH5Rz/sCnzWij4iINKN4EvoHQH8zyzaz9sAVwMvV+rwMXBu522U4UOzu2xMcq4iI1KHeiUXuXmZmNwKvAW2B+e6+wcymRdrnAUuBccBmoBT4QX3HXb169W4z+0cj484Edjdy35aQ6vFB6seo+JpG8TVNKsf31doazL3Gpe6UZ2b57p6b7Dhqk+rxQerHqPiaRvE1TarHVxvNFBURCQkldBGRkGitCf2xZAdQj1SPD1I/RsXXNIqvaVI9vpha5TV0ERGpqbWO0EVEpBoldBGRkEjphJ7KZXvNrJ+ZvWlmG81sg5n9nxh9RptZsZmtjSyzWiq+yOtvNbO/RV47P0Z7Ms/fgKjzstbM9pnZjGp9Wvz8mdl8M9tpZuujtp1gZn8xs08ij1+pZd86P6/NGN/9ZrYp8t9wkZl1r2XfOj8PzRjfbDPbFvXfcVwt+ybr/D0TFdtWM1tby77Nfv6azN1TciGYxLQF+BrQHlgHnF6tzzjgFYJaMsOBlS0YX2/grMh6V+DjGPGNBpYk8RxuBTLraE/a+Yvx3/pz4KvJPn/AucBZwPqobfcBd0TW7wB+Vct7qPPz2ozxXQS0i6z/KlZ88XwemjG+2cDMOD4DSTl/1dofAGYl6/w1dUnlEXpKl+119+3uviayvh/YSIwKkykuVcoenw9scffGzhxOGHdfDuyttvkS4InI+hPAd2LsGs/ntVnic/dl7l4Wefo+QS2lpKjl/MUjaeevgpkZMBl4KtGv21JSOaEnrGxvczOzLOBMYGWM5hFmts7MXjGzgS0bGQ4sM7PVZjY1RntKnD+C+kC1/U+UzPNX4SSP1CaKPJ4Yo0+qnMsfEvzVFUt9n4fmdGPkktD8Wi5ZpcL5OwfY4e6f1NKezPMXl1RO6Akr29uczKwL8AIww933VWteQ3AZ4QzgIeDFlowNGOXuZxH8otQNZnZutfZUOH/tgYnAczGak33+GiIVzuVPgDJgYS1d6vs8NJe5wL8AOcB2gssa1SX9/AFXUvfoPFnnL26pnNBTvmyvmWUQJPOF7v7n6u3uvs/dSyLrS4EMM8tsqfjc/bPI405gEcGftdFSoezxWGCNu++o3pDs8xdlR8WlqMjjzhh9kv1Z/D4wHrjaIxd8q4vj89As3H2Hu5e7+1Hgd7W8brLPXzvgu8AztfVJ1vlriFRO6Cldtjdyve1xYKO7/3stfXpF+mFmQwnO954Wiq+zmXWtWCf44mx9tW6pUPa41lFRMs9fNS8D34+sfx94KUafeD6vzcLMxgC3AxPdvbSWPvF8HporvujvZf61ltdN2vmLuADY5O5FsRqTef4aJNnfyta1ENyF8THBt98/iWybBkyLrBvBD1hvAf4G5LZgbN8k+JPwQ2BtZBlXLb4bgQ0E39i/D4xswfi+FnnddZEYUur8RV6/E0GCPj5qW1LPH8E/LtuBIwSjxv8F9ADeAD6JPJ4Q6XsysLSuz2sLxbeZ4PpzxedwXvX4avs8tFB8f4x8vj4kSNK9U+n8RbYvqPjcRfVt8fPX1EVT/0VEQiKVL7mIiEgDKKGLiISEErqISEgooYuIhIQSuohISCihS6tnZiWRxywzuyrBx76r2vMViTy+SCIpoUuYZAENSuhm1raeLlUSuruPbGBMIi1GCV3C5F7gnEi96h+bWdtIrfAPIoWhroPKOutvmtmfCCa8YGYvRooubagovGRm9wIdI8dbGNlW8deARY69PlIj+/KoY+eZ2fMW1ChfWDHbVaS5tUt2ACIJdAdB3e3xAJHEXOzuQ8zsOOBdM1sW6TsUGOTun0ae/9Dd95pZR+ADM3vB3e8wsxvdPSfGa32XoNjUGUBmZJ/lkbYzgYEEtUjeBUYB7yT6zYpUpxG6hNlFBLVq1hKUNu4B9I+0rYpK5gA3m1lFiYF+Uf1q803gKQ+KTu0A3gKGRB27yINiVGsJLgWJNDuN0CXMDLjJ3V+rstFsNPBltecXACPcvdTM8oAOcRy7Noei1svR/2fSQjRClzDZT/BzgBVeA66PlDnGzE6NVMqr7njgi0gy/zrBz/FVOFKxfzXLgcsj1+l7Evy02aqEvAuRRtLIQcLkQ6AsculkAfAbgssdayJfTO4i9s/HvQpMM7MPgY8ILrtUeAz40MzWuPvVUdsXASMIqu858H/d/fPIPwgiSaFqiyIiIaFLLiIiIaGELiISEkroIiIhoYQuIhISSugiIiGhhC4iEhJK6CIiIfH/AV9Z/mQM8/dwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# template for Problem 2(b)\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### Simulate data:\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "theta_true = 3\n",
    "tau_true = 0.5\n",
    "n_samples = 100\n",
    "\n",
    "x = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    # Sample from N(0,1) or N(theta_true,1)\n",
    "    if np.random.rand() < 1 - tau_true:\n",
    "        x[i] = np.random.normal(0, 1)\n",
    "    else:\n",
    "        x[i] = np.random.normal(theta_true, 1)\n",
    "\n",
    "\n",
    "### The EM algorithm:\n",
    "\n",
    "n_iter = 20\n",
    "theta = np.zeros(n_iter)\n",
    "tau = np.zeros(n_iter)\n",
    "\n",
    "# Initial guesses for theta and tau\n",
    "theta[0] = 1\n",
    "tau[0] = 0.1\n",
    "\n",
    "for it in range(1, n_iter):\n",
    "    # The current estimates for theta and tau,\n",
    "    # computed in the previous iteration\n",
    "    theta_0 = theta[it-1]\n",
    "    tau_0 = tau[it-1]\n",
    "\n",
    "    # E-step: compute the responsibilities r1 and r2\n",
    "    # r1 = ?\n",
    "    # r2 = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    r1_unnorm = (1-tau_0)*scipy.stats.norm.pdf(x, 0, 1)\n",
    "    r2_unnorm = tau_0*scipy.stats.norm.pdf(x, theta_0, 1)\n",
    "    r2 = r2_unnorm / (r1_unnorm + r2_unnorm)\n",
    "\n",
    "    # M-step: compute the parameter values that maximize\n",
    "    # the expectation of the complete-data log-likelihood.\n",
    "    # theta[it] = ?\n",
    "    # tau[it] = ?\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    theta[it] = sum(r2 * x) / sum(r2)\n",
    "    tau[it] = sum(r2) / n_samples\n",
    "    \n",
    "\n",
    "# Print and plot the values of theta and tau in each iteration\n",
    "print(\"theta       tau\")\n",
    "for theta_i, tau_i in zip(theta, tau):\n",
    "    print(\"{0:.7f}  {1:.7f}\".format(theta_i, tau_i))\n",
    "\n",
    "plt.plot(range(n_iter), theta, 'b-', label = 'theta')\n",
    "plt.plot(range(n_iter), [theta_true]*n_iter, 'b--', label = 'true theta')\n",
    "plt.plot(range(n_iter), tau, 'g-', label = 'tau')\n",
    "plt.plot(range(n_iter), [tau_true]*n_iter, 'g--', label = 'true tau')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82083e7d021d449db017e6010251e7ad",
     "grade": false,
     "grade_id": "cell-482274cb8fbd6887",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Problem 3: PyTorch\n",
    "Go through the PyTorch tutorials in the three links and answer the questions given below\n",
    "\n",
    "1) What is PyTorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "2) Autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "3) Linear regression with PyTorch: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "\n",
    "__(a)__ What are PyTorch Tensors and how do you run a CPU tensor on GPU? \n",
    "\n",
    "\n",
    "__(b)__ What is Automatic differentiation and autograd? \n",
    "\n",
    "\n",
    "__(c)__ PyTorch constructs the computation graph dynamically as the operations are defined. In the 'linear regression with PyTorch' tutorial which line numbers indicates the completion of the computation graph, computation of the gradients and update of the weights, respectively? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to Problem 3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "__(a)__ \n",
    "\n",
    "Pytorch Tensors is a data structure similar to arrays and matrices used to encode the inputs and outputs of a model, as well as model's parameters.\n",
    "\n",
    "To run a CPU tensor on GPU you should first run the following code:\n",
    "\n",
    "```Python\n",
    "    # We move our tensor to the GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.to('cuda')\n",
    "```\n",
    "\n",
    "And then use the tensor operations as in a CPU.\n",
    "\n",
    "__(b)__\n",
    "\n",
    "Automatic differentation (autodiff) refers to a general way of taking a program which computes a value, and automatically constructing a procedure for computing derivatives of that value.\n",
    "[REF](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf \"Automatic Differentiation\")\n",
    "\n",
    "Autograd is a particular autdiff package used by PyTorch to power neural network training.\n",
    "\n",
    "__(C)__\n",
    "\n",
    "Completion of the computation graph: Lines 30 to 42.\n",
    "\n",
    "Computation of the gradients: Line 41: ```loss.backward()```\n",
    "\n",
    "Update of the weights: Line 42: ```optimizer.step()```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
