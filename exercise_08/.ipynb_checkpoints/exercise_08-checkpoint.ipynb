{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d263736b67774a0e03ab635f7cd4dfd5",
     "grade": false,
     "grade_id": "cell-7e3f1e2b3d7409fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CS-E4820 Machine Learning: Advanced Probabilistic Methods (spring 2021)\n",
    "\n",
    "Pekka Marttinen, Santosh Hiremath, Tianyu Cui, Yogesh Kumar, Zheyang Shen, Alexander Aushev, Khaoula El Mekkaoui, Shaoxiong Ji, Alexander Nikitin, Sebastiaan De Peuter, Joakim JÃ¤rvinen.\n",
    "\n",
    "## Exercise 8, due on Tuesday March 30 at 23:00.\n",
    "\n",
    "### Contents\n",
    "1. Problem 1: Minimize KL divergence using PyTorch\n",
    "2. Problem 2: VB for a factor analysis model (1/2)\n",
    "3. Problem 3: VB for a factor analysis model (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48aefe82a5afe18f9185bfb71bbfb637",
     "grade": false,
     "grade_id": "cell-8f76ca03c69cb4d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 1: Minimize KL divergence using PyTorch\n",
    "PyTorch is a powerful auto-differentiation framework that allows us to do any optimization, as long as we can define the objective function and corresponding optimization variables. It has been widely used for Bayesian deep learning. In this exercise, we will study how to use PyTorch to fit a Gaussian distribution to a known Mixture of Gaussian by minimizing their KL divergence, and compare the difference between the forward and reverse form of the KL.\n",
    "\n",
    "Recall that the KL divergence between two distributions $q(x)$ and $p(x)$ is defined as:\n",
    "\n",
    "$$\\text{KL}[q(x)|p(x)]=\\int q(x)\\log\\frac{q(x)}{p(x)}dx.$$\n",
    "\n",
    "This is typically called the **Reverse KL** which we have used before in the course (like in Variational Bayes). If the probability density functions of $q(x)$ and $p(x)$ are known, and we can get samples from $q(x)$, an unbiased estimator of KL divergence is:\n",
    "$$\\text{KL}[q(x)|p(x)]\\approx\\log\\frac{q(x_i)}{p(x_i)}=\\log q(x_i)-\\log p(x_i),$$\n",
    "where $x_i\\sim q(x)$. We will use above estimator for this exercise.\n",
    "\n",
    "There is also a **Forward KL**: $\\text{KL}[p(x)|q(x)]$ defined as:\n",
    "\n",
    "$$\\text{KL}[p(x)|q(x)]=\\int p(x)\\log\\frac{p(x)}{q(x)}dx,$$\n",
    "\n",
    "which is used in other inference algorithms such as Expectration Propogation which is not within the scope of this course.\n",
    "\n",
    "Let $p(x \\mid \\pi) = \\pi \\mathcal{N}(0,1)+(1-\\pi)\\mathcal{N}(8,1)$ where $\\pi\\sim\\text{Bernoulli}(0.4)$ be the true mixture distribution which we want to fit using a Gaussian $q(x; \\mu, \\sigma)$. We want to estimate $\\mu$ and $\\sigma$ using both the forwared and reverse KL.\n",
    "\n",
    "Complete the template below with the relevant code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d052f67014581203e59c5bf9d58823cc",
     "grade": false,
     "grade_id": "cell-33040ac0aa3bb106",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing reverse KL\n",
      "EPOACH 1: KL: 3.1620.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOACH 101: KL: 0.2682.\n",
      "EPOACH 201: KL: 0.3176.\n",
      "EPOACH 301: KL: 0.3006.\n",
      "EPOACH 401: KL: 0.3070.\n",
      "EPOACH 501: KL: 0.3247.\n",
      "EPOACH 601: KL: 0.3291.\n",
      "EPOACH 701: KL: 0.3080.\n",
      "EPOACH 801: KL: 0.2978.\n",
      "EPOACH 901: KL: 0.3168.\n",
      "Optimizing forward KL\n",
      "EPOACH 1: KL: -0.1914.\n",
      "EPOACH 101: KL: -0.3619.\n",
      "EPOACH 201: KL: -0.3770.\n",
      "EPOACH 301: KL: -0.4165.\n",
      "EPOACH 401: KL: -0.3940.\n",
      "EPOACH 501: KL: -0.3540.\n",
      "EPOACH 601: KL: -0.3717.\n",
      "EPOACH 701: KL: -0.3852.\n",
      "EPOACH 801: KL: -0.3653.\n",
      "EPOACH 901: KL: -0.3681.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f22397904f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as Dis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Gaussian:\n",
    "    \"\"\"\n",
    "    This represents q(x) \n",
    "    Gaussian distribution is parametrized by mean (mu) and standard deviation. The standard deviation is \n",
    "    parametrized as sigma = log(1 + exp(rho)) to make it positive all the time. A sample from the distribution\n",
    "    can be obtained by first sampling from a unit Gaussian, shifting the samples by the mean and scaling by the \n",
    "    standard deviation: w = mu + log(1 + exp(rho)) * epsilon\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mean = mu\n",
    "        self.rho = rho\n",
    "\n",
    "    @property\n",
    "    def std_dev(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from Gaussian distribution\n",
    "        # Return a tensor contains all the samples \n",
    "        \n",
    "        # Sample num_samples datapoints from N(0,1) \n",
    "        epsilon = Dis.Normal(0,1).sample([num_samples])\n",
    "        \n",
    "        # Scale and shift epsilon\n",
    "        # samples = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.mean + self.std_dev * epsilon\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        # Compute the log probability of each sample under Gaussian distribution\n",
    "        # Return a tensor containing the log probability of all samples        \n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        # YOUR CODE HERE\n",
    "        logp = -1/2 * torch.log(2*np.pi*self.rho) -1/2*((samples-self.mean)/self.rho)**2\n",
    "        #print('logp:', logp)\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return logp\n",
    "    \n",
    "class MoG:\n",
    "    \"\"\"\n",
    "    This represents p(x).\n",
    "    In this example, mixture of two Gaussian distribution is constructed by 2 Gaussian distributions \n",
    "    N(0,2) and N(8,1), and each datapoint is from N(0,2) with probability p = 0.4 and from N(8,1) with \n",
    "    probability 0.6.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_1=0., sigma_1=1., mu_2=8., sigma_2=1., prob = 0.4):\n",
    "        self.mean_1 = torch.tensor(mu_1)\n",
    "        self.sigma_1 = torch.tensor(sigma_1)\n",
    "        self.mean_2 = torch.tensor(mu_2)\n",
    "        self.sigma_2 = torch.tensor(sigma_2)\n",
    "        self.prob = torch.tensor(prob)\n",
    "\n",
    "    def sample(self, num_samples = 1):\n",
    "        # Sample num_samples data points from MoG distribution\n",
    "        # Return a tensor contains all the samples\n",
    "        \n",
    "        # sample from N(0, 2)\n",
    "        # sample form N(8, 1)\n",
    "        # sample from Bern(0.4)\n",
    "        # Combine the three to from a sample form mixture\n",
    "        # sample_gaussian_1 = ? # EXERCISE\n",
    "        # sample_gaussian_2 = ? # EXERCISE\n",
    "        # sample_bernoulli = ? # EXERCISE\n",
    "        # samples  = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        epsilon = Dis.Normal(0,1).sample([num_samples])\n",
    "        sample_gaussian_1 = self.mean_1 + torch.log(1+torch.exp(self.sigma_1))*epsilon\n",
    "        sample_gaussian_2 = self.mean_2 + torch.log(1+torch.exp(self.sigma_2))*epsilon\n",
    "        sample_bernoulli = Dis.Bernoulli(self.prob).sample([num_samples])\n",
    "        samples = sample_bernoulli*sample_gaussian_1 + (1-sample_bernoulli)*sample_gaussian_2\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def logprob(self, samples):\n",
    "        \n",
    "        # Compute the log probability of each sample under the MoG distribution\n",
    "        # Return a tensor containing the log probability of all samples\n",
    "        \n",
    "        # logp = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        N1 = 1/(self.sigma_1*np.sqrt(2*np.pi)) * torch.exp(-1/2 * ((samples-self.mean_1)/self.sigma_1)**2)\n",
    "        N2 = 1/(self.sigma_2*np.sqrt(2*np.pi)) * torch.exp(-1/2 * ((samples-self.mean_2)/self.sigma_2)**2)\n",
    "        #print('N2',N2)\n",
    "        logp = torch.log(self.prob*N1 + (1-self.prob)*N2)\n",
    "        #print('logp:', logp)\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return logp    \n",
    "\n",
    "class KL_divergence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KL_divergence, self).__init__()\n",
    "        # define the mean and standard deviation as parameters, and initialization\n",
    "        self.mu = nn.Parameter(torch.Tensor(1).uniform_(-2., 12.))\n",
    "        self.rho = nn.Parameter(torch.Tensor(1).uniform_(1.0, 5.0))\n",
    "        \n",
    "        self.gaussian = Gaussian(self.mu, self.rho)\n",
    "        self.mog = MoG()\n",
    "    \n",
    "    def compute_forwardKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        \n",
    "        # compute the forward KL divergence between p and q of num_samples data points\n",
    "        # Return the estimated forward KL divergence\n",
    "        \n",
    "        \n",
    "        # sample form MoG\n",
    "        # compute forware KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # fkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.mog.sample(num_samples)\n",
    "        #print('MOG:',self.mog.logprob(samples))\n",
    "        fkl = torch.mean(self.mog.logprob(samples)-self.gaussian.logprob(samples))\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return fkl\n",
    "    \n",
    "    def compute_reverseKL(self):\n",
    "        num_samples = torch.tensor(1000)\n",
    "        # compute the reverse KL divergence between p and q with num_samples data points\n",
    "        # Return the estimated reverse KL divergence\n",
    "        \n",
    "        # sample form Gaussian\n",
    "        # compute reverse KL \n",
    "        \n",
    "        # samples = ? # EXERCISE\n",
    "        # rkl = ? # EXERCISE\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        samples = self.gaussian.sample(num_samples)\n",
    "        aux = self.gaussian.logprob(samples)-self.mog.logprob(samples)\n",
    "        #print('aux:',aux)\n",
    "        rkl = torch.mean(aux)\n",
    "        #raise NotImplementedError()\n",
    "        return rkl\n",
    "\n",
    "# Optimize the KL by using gradient descent\n",
    "def optimization(kl, forward = False, learning_rate = 0.1, num_epoch = 1000):\n",
    "    parameters = set(kl.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate, eps=1e-3)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        if forward:\n",
    "            loss = kl.compute_forwardKL()\n",
    "        else:\n",
    "            loss = kl.compute_reverseKL()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch % 100) == 0:\n",
    "            print('EPOACH %d: KL: %.4f.'% (epoch+1, loss))\n",
    "\n",
    "print('Optimizing reverse KL')\n",
    "torch.manual_seed(0)\n",
    "kl_reverse = KL_divergence()\n",
    "optimization(kl_reverse, forward = False)\n",
    "Gaussian_reverse= kl_reverse.gaussian\n",
    "\n",
    "print('Optimizing forward KL')\n",
    "kl_forward = KL_divergence()\n",
    "optimization(kl_forward, forward= True)\n",
    "Gaussian_forward = kl_forward.gaussian\n",
    "\n",
    "# Plot the pdf of Gaussian fitted from forward KL and reverse KL, and also the ground truth pdf from MoG\n",
    "x_plot = torch.linspace(-5., 15., 1000)\n",
    "density_mog = torch.exp(kl_forward.mog.logprob(x_plot)).detach().numpy()\n",
    "#density_gaussian = torch.exp(kl_forward.gaussian.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_forward = torch.exp(Gaussian_forward.logprob(x_plot)).detach().numpy()\n",
    "density_Gaussian_reverse = torch.exp(Gaussian_reverse.logprob(x_plot)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_plot, density_mog)\n",
    "#ax.plot(x_plot, density_gaussian)\n",
    "ax.plot(x_plot, density_Gaussian_forward)\n",
    "ax.plot(x_plot, density_Gaussian_reverse)\n",
    "\n",
    "ax.legend(('MoG Density', 'Forward KL', 'Reverse KL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9b26bff6d2b8ec275fbb86082cee644",
     "grade": false,
     "grade_id": "cell-c9f75665b42eb29f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 2: VB for a factor analysis model (1/2)\n",
    "\n",
    "The data set consists of $D$-dimensional vectors $\\mathbf{x}_{n}\\in \\mathbb{R}^{D},$ for $n=1,\\ldots,N$. We model the data using factor analysis with $K$-dimensional factors $\\mathbf{z}_{n}\\in\\mathbb{R}^{K}$. In detail, the\n",
    "model is specified as follows:\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_{n} &  \\sim\\mathcal{N}_{D}(\\mathbf{Wz}_{n},\\text{diag}\n",
    "(\\mathbf{\\psi})^{-1}),\\quad n=1,\\ldots,N,\\\\\n",
    "\\psi_{d} &  \\sim\\text{Gamma}(a,b),\\quad d=1,\\ldots,D,\\\\\n",
    "\\mathbf{w}_{d} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,}\\alpha\\mathbf{I}),\\quad\n",
    "d=1,\\ldots,D,\\\\\n",
    "\\mathbf{z}_{n} &  \\sim\\mathcal{N}_{K}(\\mathbf{0,I}),\\quad n=1,\\ldots,N.\n",
    "\\end{align*}\n",
    "Here, $\\mathbf{W}$ is a $D\\times K$ factor loading matrix and $\\mathbf{w}_{d}$ is the $d$th row of $\\mathbf{W}$ written as a column vector. Parameter $\\psi_{d}^{-1}$ is the variance for the $d$th dimension in the observed data\n",
    "and diag$(\\psi)$ denotes a diagonal matrix with elements $\\mathbf{\\psi} =(\\psi_{1},\\ldots,\\psi_{D})^{T}$ on the diagonal.\n",
    "\n",
    "We approximate the posterior $p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W} |\\mathbf{X})$ using the mean-field approximation: \n",
    "\n",
    "$$ \n",
    "q(\\Theta)=\\prod_{d=1}^{D}q(\\mathbf{w}_{d})\\prod_{n=1}^{N}q(\\mathbf{z}_{n})\\prod_{d=1}^{D}q(\\psi_{d}).\n",
    "$$\n",
    "\n",
    "\n",
    "__1__ Write the logarithm of the joint distribution, $\\log p(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X})$.\n",
    "\n",
    "__2__ Remove from the logarithm of the joint distribution all terms that do not depend on $\\mathbf{z}_{n}$.\n",
    "\n",
    "__3__ Show that the updated factor $q(\\mathbf{z}_{n})$ is equal to\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z}_{n})=\\mathcal{N}_{K}(\\mathbf{\\mu}_{n},\\mathbf{K}_{n}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{K}_{n} &  =\\left[  I+\\sum_{d=1}^{D}\\left\\langle \\psi_{d}\\right\\rangle\n",
    "\\left\\langle \\mathbf{w}_{d}\\mathbf{w}_{d}^{T}\\right\\rangle \\right]  ^{-1}\n",
    "\\quad\\text{and}\\\\\n",
    "\\mathbf{\\mu}_{n} &  =\\mathbf{K}_{n}\\left\\langle \\mathbf{W}^{T}\\right\\rangle\n",
    "\\text{diag}(\\left\\langle \\mathbf{\\psi}\\right\\rangle )\\mathbf{x}_{n}.\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\left\\langle \\mathbf{\\cdot}\\right\\rangle $ is used as a shorthand for the expectation of a variable with respect to its factor, e.g., $\\left\\langle \\mathbf{\\psi}\\right\\rangle =\\mathbb{E}_{q(\\mathbf{\\psi})}[\\mathbf{\\psi}]$ etc.\n",
    "\n",
    "__Hint 1:__ Try to write the log joint as \n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\mathbf{z}_{n}^{T}\\mathbf{Az}_{n}+\\mathbf{b}^{T}\\mathbf{z}_{n}\n",
    "$$\n",
    "\n",
    "for some $\\mathbf{A}$ and $\\mathbf{b}$, after which you can apply the 'completing the square' technique.\n",
    "\n",
    "__Hint 2:__ Suppose $\\mathbf{A}$ is an $N\\times M$ matrix. Further suppose that $\\mathbf{D}$ is an $N\\times N$ diagonal matrix, $\\mathbf{D} =$diag$(d_{1},\\ldots,d_{N})$. Then $\\mathbf{A}^{T}\\mathbf{DA}$ can be written\n",
    "as\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{T}\\mathbf{DA=}\\sum_{n=1}^{N}d_{n}\\mathbf{a}_{n}\\mathbf{a}_{n}^{T},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}_{n}$ is the $n$th row of $\\mathbf{A}$ written as a column vector.\n",
    "\n",
    "__Hint 3:__ Recall that expectation is a linear operator, i.e. $\\mathbb{E}(aX+bY)=a\\mathbb{E}(X)+b\\mathbb{E}(Y)$. Further, if some random variables $A$ and $B$ are independent, then $\\mathbb{E}_{q(A)q(B)} (AB)=\\mathbb{E}_{q(A)}(A)\\mathbb{E}_{q(B)}(B)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "__1:__\n",
    "\n",
    "First of all we know that the joint distrbution can be expressed as:\n",
    "\n",
    "$$p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) = \\prod_{n=1}^N p\\left(\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W},\\mathbf{x}_n\\right)$$\n",
    "\n",
    "And descomposing following the dag we have that:\n",
    "\n",
    "$$p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) = \\prod_{n=1}^N p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right)p\\left(\\mathbf{\\psi}\\right)p\\left(\\mathbf{z}_n\\right)p\\left(\\mathbf{W}\\right)$$\n",
    "\n",
    "Therefore the logarithm will be:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) = \\log \\left( \\prod_{n=1}^N p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right)p\\left(\\mathbf{\\psi}\\right)p\\left(\\mathbf{z}_n\\right)p\\left(\\mathbf{W}\\right) \\right) = \\sum_{n=1}^N \\log \\left( p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right)p\\left(\\mathbf{\\psi}\\right)p\\left(\\mathbf{z}_n\\right)p\\left(\\mathbf{W}\\right) \\right) = $$\n",
    "$$= \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\log p\\left(\\mathbf{\\psi}\\right) + \\log p\\left(\\mathbf{z}_n\\right) + \\log p\\left(\\mathbf{W}\\right)$$\n",
    "\n",
    "__2:__\n",
    "\n",
    "Now, if we remove all the elements that don't depend on $\\mathbf{z}_n$ we obtain:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) \\propto \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\log p\\left(\\mathbf{z}_n\\right)$$\n",
    "\n",
    "Then, knowing that:\n",
    "\n",
    "$$\\mathbf{x}_n \\sim \\mathcal{N}_{D}\\left(\\mathbf{Wz}_n,\\text{diag}\\left(\\mathbf{\\psi}\\right)^{-1}\\right) = \\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) \\right),\\quad n=1,\\ldots,N$$\n",
    "$$\\mathbf{z}_n \\sim \\mathcal{N}_K\\left(\\mathbf{0,I}\\right) = \\exp\\left(-\\frac{1}{2}\\mathbf{z}_n^\\top \\mathbf{z}_n \\right),\\quad n=1,\\ldots,N$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) \\propto \\sum_{n=1}^N \\log \\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) \\right) + \\log \\exp\\left(-\\frac{1}{2}\\mathbf{z}_n^\\top \\mathbf{z}_n \\right) =$$\n",
    "$$= \\sum_{n=1}^N \\left(-\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) \\right) + \\left(-\\frac{1}{2}\\mathbf{z}_n^\\top \\mathbf{z}_n \\right) = \\sum_{n=1}^N -\\frac{1}{2} \\left( \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) + \\mathbf{z}_n^\\top \\mathbf{z}_n \\right) =$$\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2} \\left(\\left(\\mathbf{x}_n^\\top - \\mathbf{z}_n^\\top\\mathbf{W}^\\top\\right) \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) + \\mathbf{z}_n^\\top \\mathbf{z}_n\\right) = \\sum_{n=1}^N -\\frac{1}{2} \\left(\n",
    "    \\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n \n",
    "    - 2\\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\mathbf{z}_n^\\top\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\mathbf{z}_n^\\top \\mathbf{z}_n\n",
    "\\right)$$\n",
    "\n",
    "Dropping the elements that don't depend on $\\mathbf{z}_n$ and solving we continue as:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) \\propto  \\sum_{n=1}^N -\\frac{1}{2} \\left(\n",
    "    - 2\\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W}\\mathbf{z}_n\n",
    "    + \\mathbf{z}_n^\\top\\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W}+ \\mathbf{I}\\right)\\mathbf{z}_n\n",
    "\\right) =$$\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2} \\mathbf{z}_n^\\top\\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W}+ \\mathbf{I}\\right)\\mathbf{z}_n\n",
    "+ \\left(\\left(\\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W}\\right)^\\top\\right)^\\top\\mathbf{z}_n =$$\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2} \\mathbf{z}_n^\\top\\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} + \\mathbf{I}\\right)\\mathbf{z}_n\n",
    "+ \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right)^\\top\\mathbf{z}_n =$$\n",
    "\n",
    "Now applying the completing the square technique we obtain:\n",
    "\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2} \\left(\\mathbf{z}_n - \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right)^\\top \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} + \\mathbf{I}\\right) \\left(\\mathbf{z}_n - \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right) +$$ \n",
    "$$+ \\frac{1}{2}\\left(\\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right)^\\top \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right) \\propto$$\n",
    "\n",
    "Finally, we can drop the last term since it doesn't depend on $\\mathbf{z}_n$ and knowing that $\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{W} = \\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top$ we get:\n",
    "\n",
    "$$\\propto \\sum_{n=1}^N -\\frac{1}{2} \\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right)^\\top \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)$$\n",
    "$$\\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right)$$\n",
    "\n",
    "__3:__\n",
    "\n",
    "$$\\log q\\left(\\mathbf{z}_n\\right) = E_{q\\left(\\mathbf{W}\\right)q\\left(\\mathbf{\\psi}\\right)}\\left[\\log p\\left(\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W},\\mathbf{x_n}\\right)\\right] \\propto$$\n",
    "$$\\propto E_{q\\left(\\mathbf{W}\\right)q\\left(\\mathbf{\\psi}\\right)} \\Bigg[-\\frac{1}{2} \\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right)^\\top \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)$$\n",
    "$$\\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\psi_d \\mathbf{w}_d \\mathbf{w}_d^\\top + \\mathbf{I}\\right)^{-1} \\left(\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n\\right) \\right)\\Bigg] =$$\n",
    "$$= -\\frac{1}{2} \\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\psi_d\\right] E_{q\\left(\\mathbf{W}\\right)}\\left[\\mathbf{w}_d \\mathbf{w}_d^\\top\\right] + \\mathbf{I}\\right)^{-1} \\left(E_{q\\left(\\mathbf{W}\\right)}\\left[\\mathbf{W}^\\top\\right] \\text{diag}\\left(E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\mathbf{\\psi}\\right]\\right) \\mathbf{x}_n\\right) \\right)^\\top \\left(\\sum_{d=1}^D E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\psi_d\\right] E_{q\\left(\\mathbf{W}\\right)}\\left[\\mathbf{w}_d \\mathbf{w}_d^\\top\\right] + \\mathbf{I}\\right)$$\n",
    "$$\\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\psi_d\\right] E_{q\\left(\\mathbf{W}\\right)}\\left[\\mathbf{w}_d \\mathbf{w}_d^\\top\\right] + \\mathbf{I}\\right)^{-1} \\left(E_{q\\left(\\mathbf{W}\\right)}\\left[\\mathbf{W}^\\top\\right] \\text{diag}\\left(E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\mathbf{\\psi}\\right]\\right) \\mathbf{x}_n\\right) \\right) =$$\n",
    "\n",
    "Applying the given notation for $\\left\\langle \\mathbf{\\cdot}\\right\\rangle$\n",
    "\n",
    "$$= -\\frac{1}{2} \\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\left\\langle\\psi_d\\right\\rangle \\left\\langle\\mathbf{w}_d \\mathbf{w}_d^\\top\\right\\rangle + \\mathbf{I}\\right)^{-1} \\left(\\left\\langle\\mathbf{W}^\\top\\right\\rangle \\text{diag}\\left(\\left\\langle\\mathbf{\\psi}\\right\\rangle\\right) \\mathbf{x}_n\\right) \\right)^\\top \\left(\\sum_{d=1}^D \\left\\langle\\psi_d\\right\\rangle \\left\\langle\\mathbf{w}_d \\mathbf{w}_d^\\top\\right\\rangle + \\mathbf{I}\\right)$$\n",
    "$$\\left(\\mathbf{z}_n - \\left(\\sum_{d=1}^D \\left\\langle\\psi_d\\right\\rangle \\left\\langle\\mathbf{w}_d \\mathbf{w}_d^\\top\\right\\rangle + \\mathbf{I}\\right)^{-1} \\left(\\left\\langle\\mathbf{W}^\\top\\right\\rangle \\text{diag}\\left(\\left\\langle\\mathbf{\\psi}\\right\\rangle\\right) \\mathbf{x}_n\\right) \\right)$$\n",
    "\n",
    "We observe how:\n",
    "$$q(\\mathbf{z}_{n})=\\mathcal{N}_{K}(\\mathbf{\\mu}_{n},\\mathbf{K}_{n})$$\n",
    "where \n",
    "$\\mathbf{K}_{n} = \\left(\\sum_{d=1}^D \\left\\langle\\psi_d\\right\\rangle \\left\\langle\\mathbf{w}_d \\mathbf{w}_d^\\top\\right\\rangle + \\mathbf{I}\\right)^{-1}$\n",
    ", and \n",
    "$\\mathbf{\\mu}_{n} = \\mathbf{K}_{n} \\left\\langle\\mathbf{W}^\\top\\right\\rangle \\text{diag}\\left(\\left\\langle\\mathbf{\\psi}\\right\\rangle\\right) \\mathbf{x}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "337e6c54afe83f1330cba6cf779db1aa",
     "grade": false,
     "grade_id": "cell-fd4f81c2e2c0650f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Problem 3: VB for a factor analysis model (2/2)\n",
    "For the factor analysis model considered in Problem 2, derive the update for factor $q(\\mathbf{w}_{d})$. The updated factor should be given in terms of the following expectations: $\\left\\langle \\psi_{d}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\right\\rangle ,\\left\\langle \\mathbf{z}_{n}\\mathbf{z}_{n}^{T}\\right\\rangle $, which have been calculated using the current values of the other factors for all $d,n$.\n",
    "\n",
    "__Hint__: A multivariate Gaussian with a diagonal covariance can be expressed as a product of independent univariate Gaussians, which allows you to simplify the formulas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "We will repeat a similar procedure as in exercise 2.\n",
    "\n",
    "First, we have the logarithm of the joint distribution:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) = \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\log p\\left(\\mathbf{\\psi}\\right) + \\log p\\left(\\mathbf{z}_n\\right) + \\log p\\left(\\mathbf{W}\\right)$$\n",
    "\n",
    "Then we remove all the elements that don't depend on $\\mathbf{w}_d$:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) \\propto \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\log p\\left(\\mathbf{W}\\right) =$$\n",
    "$$= \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\log \\prod_{d=1}^D p\\left(\\mathbf{w}_d\\right) =$$\n",
    "$$= \\sum_{n=1}^N \\log p\\left(\\mathbf{x}_n|\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W}\\right) + \\sum_{d=1}^D \\log p\\left(\\mathbf{w}_d\\right)$$\n",
    "\n",
    "Then knowing that:\n",
    "\n",
    "$$\\mathbf{x}_n \\sim \\mathcal{N}_{D}\\left(\\mathbf{Wz}_n,\\text{diag}\\left(\\mathbf{\\psi}\\right)^{-1}\\right) = \\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) \\right),\\quad n=1,\\ldots,N$$\n",
    "$$\\mathbf{w}_{d} \\sim \\mathcal{N}_K(\\mathbf{0,}\\alpha\\mathbf{I}) = \\exp\\left(-\\frac{1}{2}\\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\\right), \\quad d=1,\\ldots,D$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\\log p\\left(\\mathbf{\\psi},\\mathbf{Z},\\mathbf{W},\\mathbf{X}\\right) \\propto \\sum_{n=1}^N \\log \\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) \\right) + \\sum_{d=1}^D \\log \\exp\\left(-\\frac{1}{2}\\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\\right) =$$\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2}\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) + \\sum_{d=1}^D -\\frac{1}{2}\\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d =$$\n",
    "$$= \\sum_{n=1}^N -\\frac{1}{2}\\left(\\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right)^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\left(\\mathbf{x}_n - \\mathbf{Wz}_n\\right) + \\sum_{d=1}^D \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\\right) =$$\n",
    "$$\n",
    "= \\sum_{n=1}^N -\\frac{1}{2} \\left(\n",
    "    \\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{x}_n \n",
    "    - 2\\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\mathbf{z}_n^\\top\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\sum_{d=1}^D \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Dropping the elements that don't depend on $\\mathbf{w}_d$ and solving we continue as:\n",
    "\n",
    "$$\n",
    "= \\sum_{n=1}^N -\\frac{1}{2} \\left(\n",
    "    - 2\\mathbf{x}_n^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\mathbf{z}_n^\\top\\mathbf{W}^\\top \\text{diag}\\left(\\mathbf{\\psi}\\right) \\mathbf{Wz}_n\n",
    "    + \\sum_{d=1}^D \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right) =\n",
    "$$\n",
    "$$\n",
    "= \\sum_{n=1}^N -\\frac{1}{2} \\left(\n",
    "    - 2 \\sum_{d=1}^D \\mathbf{x}_{nd}^\\top \\mathbf{\\psi}_d \\mathbf{w}_d\\mathbf{z}_n\n",
    "    + \\mathbf{z}_n^\\top \\sum_{d=1}^D \\mathbf{\\psi}_d \\mathbf{w}_d \\mathbf{w}_d^\\top \\mathbf{z}_n\n",
    "    + \\sum_{d=1}^D \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right) =\n",
    "$$\n",
    "$$\n",
    "= \\sum_{n=1}^N \\sum_{d=1}^D -\\frac{1}{2} \\left(\n",
    "    - 2 \\mathbf{x}_{nd}^\\top \\mathbf{\\psi}_d \\mathbf{w}_d\\mathbf{z}_n\n",
    "    + \\mathbf{z}_n^\\top \\mathbf{\\psi}_d \\mathbf{w}_d \\mathbf{w}_d^\\top \\mathbf{z}_n\n",
    "    + \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right) =\n",
    "$$\n",
    "$$\n",
    "= \\sum_{n=1}^N \\sum_{d=1}^D \\left(\n",
    "    \\mathbf{x}_{nd}^\\top \\mathbf{\\psi}_d \\mathbf{w}_d\\mathbf{z}_n\n",
    "    -\\frac{1}{2} \\mathbf{z}_n^\\top \\mathbf{\\psi}_d \\mathbf{w}_d \\mathbf{w}_d^\\top \\mathbf{z}_n\n",
    "    -\\frac{1}{2} \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Therefore, we can compute $q(\\mathbf{w}_d)$ as:\n",
    "\n",
    "$$q(\\mathbf{w}_d) = E_{q\\left(\\mathbf{z}_n\\right)q\\left(\\mathbf{\\psi}\\right)}\\left[\\log p\\left(\\mathbf{\\psi},\\mathbf{z}_n,\\mathbf{W},\\mathbf{x_n}\\right)\\right] \\propto$$\n",
    "$$\n",
    "\\propto \n",
    "E_{q\\left(\\mathbf{z}_n\\right)q\\left(\\mathbf{\\psi}\\right)}\\left[\n",
    "    \\sum_{n=1}^N \\left(\n",
    "        \\mathbf{x}_{nd}^\\top \\mathbf{\\psi}_d \\mathbf{w}_d\\mathbf{z}_n\n",
    "        -\\frac{1}{2} \\mathbf{z}_n^\\top \\mathbf{\\psi}_d \\mathbf{w}_d \\mathbf{w}_d^\\top \\mathbf{z}_n\n",
    "        -\\frac{1}{2} \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "    \\right)\n",
    "\\right]\n",
    "=\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\sum_{n=1}^N \\left(\n",
    "    \\mathbf{x}_{nd}^\\top E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\mathbf{\\psi}_d\\right] \\mathbf{w}_d E_{q\\left(\\mathbf{z}_n\\right)}\\left[\\mathbf{z}_n\\right]\n",
    "    -\\frac{1}{2} E_{q\\left(\\mathbf{z}_n\\right)}\\left[\\mathbf{z}_n^\\top\\right] E_{q\\left(\\mathbf{\\psi}\\right)}\\left[\\mathbf{\\psi}_d\\right] \\mathbf{w}_d \\mathbf{w}_d^\\top E_{q\\left(\\mathbf{z}_n\\right)}\\left[\\mathbf{z}_n\\right]\n",
    "    -\\frac{1}{2} \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right)\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\sum_{n=1}^N \\left(\n",
    "    \\mathbf{x}_{nd}^\\top \\left\\langle \\mathbf{\\psi}_d \\right\\rangle \\mathbf{w}_d \\left\\langle \\mathbf{z}_n \\right\\rangle\n",
    "    -\\frac{1}{2} \\left\\langle \\mathbf{z}_n^\\top \\right\\rangle \\left\\langle \\mathbf{\\psi}_d \\right\\rangle \\mathbf{w}_d \\mathbf{w}_d^\\top \\left\\langle \\mathbf{z}_n \\right\\rangle\n",
    "    -\\frac{1}{2} \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right)\n",
    "=\n",
    "$$\n",
    "$$\n",
    "= \n",
    "\\sum_{n=1}^N \\left(\n",
    "    \\mathbf{x}_{nd}^\\top \\left\\langle \\mathbf{\\psi}_d \\right\\rangle \\mathbf{w}_d \\left\\langle \\mathbf{z}_n \\right\\rangle\n",
    "    -\\frac{1}{2} \\left\\langle \\mathbf{\\psi}_d \\right\\rangle \\mathbf{w}_d \\mathbf{w}_d^\\top \\left\\langle \\mathbf{z}_n \\mathbf{z}_n^\\top \\right\\rangle\n",
    "    -\\frac{1}{2} \\mathbf{w}_d^\\top\\alpha^{-1}\\mathbf{I}\\mathbf{w}_d\n",
    "\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
